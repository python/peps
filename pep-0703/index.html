
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="color-scheme" content="light dark">
    <title>PEP 703 – Making the Global Interpreter Lock Optional in CPython | peps.python.org</title>
    <link rel="shortcut icon" href="../_static/py.png">
    <link rel="canonical" href="https://peps.python.org/pep-0703/">
    <link rel="stylesheet" href="../_static/style.css" type="text/css">
    <link rel="stylesheet" href="../_static/mq.css" type="text/css">
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" media="(prefers-color-scheme: light)" id="pyg-light">
    <link rel="stylesheet" href="../_static/pygments_dark.css" type="text/css" media="(prefers-color-scheme: dark)" id="pyg-dark">
    <link rel="alternate" type="application/rss+xml" title="Latest PEPs" href="https://peps.python.org/peps.rss">
    <meta property="og:title" content='PEP 703 – Making the Global Interpreter Lock Optional in CPython | peps.python.org'>
    <meta property="og:description" content="CPython’s global interpreter lock (“GIL”) prevents multiple threads from executing Python code at the same time.  The GIL is an obstacle to using multi-core CPUs from Python efficiently.  This PEP proposes adding a build configuration (--disable-gil) to...">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://peps.python.org/pep-0703/">
    <meta property="og:site_name" content="Python Enhancement Proposals (PEPs)">
    <meta property="og:image" content="https://peps.python.org/_static/og-image.png">
    <meta property="og:image:alt" content="Python PEPs">
    <meta property="og:image:width" content="200">
    <meta property="og:image:height" content="200">
    <meta name="description" content="CPython’s global interpreter lock (“GIL”) prevents multiple threads from executing Python code at the same time.  The GIL is an obstacle to using multi-core CPUs from Python efficiently.  This PEP proposes adding a build configuration (--disable-gil) to...">
    <meta name="theme-color" content="#3776ab">
</head>
<body>
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-sun-half" viewBox="0 0 24 24" pointer-events="all">
    <title>Following system colour scheme</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none"
         stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <circle cx="12" cy="12" r="9"></circle>
      <path d="M12 3v18m0-12l4.65-4.65M12 14.3l7.37-7.37M12 19.6l8.85-8.85"></path>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24" pointer-events="all">
    <title>Selected dark colour scheme</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none"
         stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24" pointer-events="all">
    <title>Selected light colour scheme</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none"
         stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
</svg>
    <script>

        document.documentElement.dataset.colour_scheme = localStorage.getItem("colour_scheme") || "auto"
    </script>
    <section id="pep-page-section">
        <header>
            <h1>Python Enhancement Proposals</h1>
            <ul class="breadcrumbs">
                <li><a href="https://www.python.org/" title="The Python Programming Language">Python</a> &raquo; </li>
                <li><a href="../pep-0000/">PEP Index</a> &raquo; </li>
                <li>PEP 703</li>
            </ul>
            <button id="colour-scheme-cycler" onClick="setColourScheme(nextColourScheme())">
                <svg aria-hidden="true" class="colour-scheme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
                <svg aria-hidden="true" class="colour-scheme-icon-when-dark"><use href="#svg-moon"></use></svg>
                <svg aria-hidden="true" class="colour-scheme-icon-when-light"><use href="#svg-sun"></use></svg>
                <span class="visually-hidden">Toggle light / dark / auto colour theme</span>
            </button>
        </header>
        <article>
            <section id="pep-content">
<h1 class="page-title">PEP 703 – Making the Global Interpreter Lock Optional in CPython</h1>
<dl class="rfc2822 field-list simple">
<dt class="field-odd">Author<span class="colon">:</span></dt>
<dd class="field-odd">Sam Gross &lt;colesbury at gmail.com&gt;</dd>
<dt class="field-even">Sponsor<span class="colon">:</span></dt>
<dd class="field-even">Łukasz Langa &lt;lukasz at python.org&gt;</dd>
<dt class="field-odd">Discussions-To<span class="colon">:</span></dt>
<dd class="field-odd"><a class="reference external" href="https://discuss.python.org/t/22606">Discourse thread</a></dd>
<dt class="field-even">Status<span class="colon">:</span></dt>
<dd class="field-even"><abbr title="Normative proposal accepted for implementation">Accepted</abbr></dd>
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><abbr title="Normative PEP with a new feature for Python, implementation change for CPython or interoperability standard for the ecosystem">Standards Track</abbr></dd>
<dt class="field-even">Created<span class="colon">:</span></dt>
<dd class="field-even">09-Jan-2023</dd>
<dt class="field-odd">Python-Version<span class="colon">:</span></dt>
<dd class="field-odd">3.13</dd>
<dt class="field-even">Post-History<span class="colon">:</span></dt>
<dd class="field-even"><a class="reference external" href="https://discuss.python.org/t/22606" title="Discourse thread">09-Jan-2023</a>,
<a class="reference external" href="https://discuss.python.org/t/26503" title="Discourse thread">04-May-2023</a></dd>
<dt class="field-odd">Resolution<span class="colon">:</span></dt>
<dd class="field-odd"><a class="reference external" href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-in-cpython-acceptance/37075">24-Oct-2023</a></dd>
</dl>
<hr class="docutils" />
<section id="contents">
<details><summary>Table of Contents</summary><ul class="simple">
<li><a class="reference internal" href="#abstract">Abstract</a></li>
<li><a class="reference internal" href="#motivation">Motivation</a><ul>
<li><a class="reference internal" href="#the-gil-makes-many-types-of-parallelism-difficult-to-express">The GIL Makes Many Types of Parallelism Difficult to Express</a></li>
<li><a class="reference internal" href="#the-gil-affects-python-library-usability">The GIL Affects Python Library Usability</a></li>
<li><a class="reference internal" href="#gpu-heavy-workloads-require-multi-core-processing">GPU-Heavy Workloads Require Multi-Core Processing</a></li>
<li><a class="reference internal" href="#the-gil-makes-deploying-python-ai-models-difficult">The GIL Makes Deploying Python AI Models Difficult</a></li>
<li><a class="reference internal" href="#motivation-summary">Motivation Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#specification">Specification</a><ul>
<li><a class="reference internal" href="#build-configuration-changes">Build Configuration Changes</a></li>
<li><a class="reference internal" href="#overview-of-cpython-changes">Overview of CPython Changes</a></li>
<li><a class="reference internal" href="#reference-counting">Reference Counting</a><ul>
<li><a class="reference internal" href="#immortalization">Immortalization</a></li>
<li><a class="reference internal" href="#biased-reference-counting">Biased Reference Counting</a><ul>
<li><a class="reference internal" href="#default-0b00">Default (<code class="docutils literal notranslate"><span class="pre">0b00</span></code>)</a></li>
<li><a class="reference internal" href="#weakrefs-0b01">Weakrefs (<code class="docutils literal notranslate"><span class="pre">0b01</span></code>)</a></li>
<li><a class="reference internal" href="#queued-0b10">Queued (<code class="docutils literal notranslate"><span class="pre">0b10</span></code>)</a></li>
<li><a class="reference internal" href="#merged-0b11">Merged (<code class="docutils literal notranslate"><span class="pre">0b11</span></code>)</a></li>
<li><a class="reference internal" href="#reference-counting-pseudo-code">Reference counting pseudo-code</a></li>
</ul>
</li>
<li><a class="reference internal" href="#deferred-reference-counting">Deferred Reference Counting</a></li>
<li><a class="reference internal" href="#garbage-collector-modifications-for-deferred-reference-counting">Garbage Collector Modifications for Deferred Reference Counting</a></li>
<li><a class="reference internal" href="#reference-counting-type-objects">Reference Counting Type Objects</a></li>
</ul>
</li>
<li><a class="reference internal" href="#memory-management">Memory Management</a><ul>
<li><a class="reference internal" href="#cpython-free-lists">CPython Free Lists</a></li>
</ul>
</li>
<li><a class="reference internal" href="#garbage-collection-cycle-collection">Garbage Collection (Cycle Collection)</a><ul>
<li><a class="reference internal" href="#stop-the-world">Stop-the-World</a></li>
<li><a class="reference internal" href="#thread-states">Thread States</a></li>
<li><a class="reference internal" href="#generations">Generations</a></li>
<li><a class="reference internal" href="#integration-with-deferred-and-biased-reference-counting">Integration With Deferred and Biased Reference Counting</a></li>
</ul>
</li>
<li><a class="reference internal" href="#container-thread-safety">Container Thread-Safety</a><ul>
<li><a class="reference internal" href="#borrowed-references">Borrowed References</a></li>
<li><a class="reference internal" href="#python-critical-sections">Python Critical Sections</a></li>
<li><a class="reference internal" href="#optimistically-avoiding-locking">Optimistically Avoiding Locking</a></li>
<li><a class="reference internal" href="#mimalloc-changes-for-optimistic-list-and-dict-access">Mimalloc Changes for Optimistic <code class="docutils literal notranslate"><span class="pre">list</span></code> and <code class="docutils literal notranslate"><span class="pre">dict</span></code> Access</a></li>
<li><a class="reference internal" href="#mimalloc-page-reuse">Mimalloc Page Reuse</a></li>
<li><a class="reference internal" href="#optimistic-dict-and-list-access-summary">Optimistic <code class="docutils literal notranslate"><span class="pre">dict</span></code> and <code class="docutils literal notranslate"><span class="pre">list</span></code> Access Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#specializing-interpreter">Specializing Interpreter</a></li>
<li><a class="reference internal" href="#py-mod-gil-slot"><code class="docutils literal notranslate"><span class="pre">Py_mod_gil</span></code> Slot</a></li>
<li><a class="reference internal" href="#pythongil-environment-variable"><code class="docutils literal notranslate"><span class="pre">PYTHONGIL</span></code> Environment Variable</a></li>
</ul>
</li>
<li><a class="reference internal" href="#rationale">Rationale</a><ul>
<li><a class="reference internal" href="#non-generational-garbage-collection">Non-Generational Garbage Collection</a></li>
<li><a class="reference internal" href="#optimistic-avoiding-locking-in-dict-and-list-accesses">Optimistic Avoiding Locking in <code class="docutils literal notranslate"><span class="pre">dict</span></code> and <code class="docutils literal notranslate"><span class="pre">list</span></code> Accesses</a></li>
</ul>
</li>
<li><a class="reference internal" href="#backwards-compatibility">Backwards Compatibility</a></li>
<li><a class="reference internal" href="#distribution">Distribution</a></li>
<li><a class="reference internal" href="#performance">Performance</a></li>
<li><a class="reference internal" href="#build-bots">Build Bots</a></li>
<li><a class="reference internal" href="#how-to-teach-this">How to Teach This</a></li>
<li><a class="reference internal" href="#reference-implementation">Reference Implementation</a></li>
<li><a class="reference internal" href="#alternatives">Alternatives</a><ul>
<li><a class="reference internal" href="#multiprocessing">Multiprocessing</a></li>
<li><a class="reference internal" href="#releasing-the-gil-in-c-api-extensions">Releasing the GIL in C-API Extensions</a></li>
<li><a class="reference internal" href="#internal-parallelization">Internal Parallelization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#related-work">Related Work</a><ul>
<li><a class="reference internal" href="#per-interpreter-gil">Per-Interpreter GIL</a></li>
<li><a class="reference internal" href="#gilectomy">Gilectomy</a></li>
<li><a class="reference internal" href="#pyparallel">PyParallel</a></li>
<li><a class="reference internal" href="#python-safethread">python-safethread</a></li>
<li><a class="reference internal" href="#greg-stein-s-free-threading-patch">Greg Stein’s Free-Threading Patch</a></li>
<li><a class="reference internal" href="#jython-and-ironpython">Jython and IronPython</a></li>
<li><a class="reference internal" href="#pypy-stm">PyPy-STM</a></li>
</ul>
</li>
<li><a class="reference internal" href="#rejected-ideas">Rejected Ideas</a><ul>
<li><a class="reference internal" href="#why-not-use-a-concurrent-garbage-collector">Why Not Use a Concurrent Garbage Collector?</a></li>
<li><a class="reference internal" href="#why-not-deprecate-pydict-getitem-in-favor-of-pydict-fetchitem">Why Not Deprecate <code class="docutils literal notranslate"><span class="pre">PyDict_GetItem</span></code> in Favor of <code class="docutils literal notranslate"><span class="pre">PyDict_FetchItem</span></code>?</a></li>
<li><a class="reference internal" href="#why-not-use-pep-683-immortalization">Why Not Use PEP 683 Immortalization?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#open-issues">Open Issues</a><ul>
<li><a class="reference internal" href="#improved-specialization">Improved Specialization</a></li>
<li><a class="reference internal" href="#python-build-modes">Python Build Modes</a></li>
<li><a class="reference internal" href="#integration">Integration</a></li>
<li><a class="reference internal" href="#mitigations-for-single-threaded-performance">Mitigations for Single-Threaded Performance</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references">References</a></li>
<li><a class="reference internal" href="#acknowledgments">Acknowledgments</a></li>
<li><a class="reference internal" href="#copyright">Copyright</a></li>
</ul>
</details></section>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Steering Council accepts PEP 703, but with clear proviso: that
the rollout be gradual and break as little as possible, and that we can roll
back any changes that turn out to be too disruptive – which includes
potentially rolling back all of PEP 703 entirely if necessary
(however unlikely or undesirable we expect that to be).</p>
</div>
<section id="abstract">
<h2><a class="toc-backref" href="#abstract" role="doc-backlink">Abstract</a></h2>
<p>CPython’s global interpreter lock (“GIL”) prevents multiple threads
from executing Python code at the same time.  The GIL is an obstacle
to using multi-core CPUs from Python efficiently.  This PEP proposes
adding a build configuration (<code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code>) to CPython to let it
run Python code without the global interpreter lock and with the
necessary changes needed to make the interpreter thread-safe.</p>
</section>
<section id="motivation">
<h2><a class="toc-backref" href="#motivation" role="doc-backlink">Motivation</a></h2>
<p>The GIL is a major obstacle to concurrency.  For scientific computing
tasks, this lack of concurrency is often a bigger issue than speed of
executing Python code, since most of the processor cycles are spent
in optimized CPU or GPU kernels.  The GIL introduces a global
bottleneck that can prevent other threads from making progress if
they call any Python code.  There are existing ways to enable
parallelism in CPython today, but those techniques come with
significant limitations (see <a class="reference internal" href="#alternatives">Alternatives</a>).</p>
<p>This section focuses on the GIL’s impact on scientific computing,
particular AI/ML workloads because that is the area with which this
author has the most experience, but the GIL also affects other users
of Python.</p>
<section id="the-gil-makes-many-types-of-parallelism-difficult-to-express">
<h3><a class="toc-backref" href="#the-gil-makes-many-types-of-parallelism-difficult-to-express" role="doc-backlink">The GIL Makes Many Types of Parallelism Difficult to Express</a></h3>
<p>Neural network-based AI models expose multiple opportunities for
parallelism.  For example, individual operations may be parallelized
internally (“intra-operator”), multiple operations may be executed
simultaneously (“inter-operator”), and requests (spanning multiple
operations) may also be parallelized.  Efficient execution requires
exploiting multiple types of parallelism <a class="footnote-reference brackets" href="#yuemmwang2019" id="id1">[1]</a>.</p>
<p>The GIL makes it difficult to express inter-operator parallelism, as
well as some forms of request parallelism, efficiently in Python. In
other programming languages, a system might use threads to run
different parts of a neural network on separate CPU cores, but this is
inefficient in Python due to the GIL. Similarly, latency-sensitive
inference workloads frequently use threads to parallelize across
requests, but face the same scaling bottlenecks in Python.</p>
<p>The challenges the GIL poses to exploiting parallelism in Python
frequently come up in reinforcement learning.  Heinrich Kuttler,
author of the NetHack Learning Environment and Member of Technical
Staff at Inflection AI, writes:</p>
<blockquote>
<div>Recent breakthroughs in reinforcement learning, such as on <a class="reference external" href="https://openai.com/five/">Dota
2</a>, <a class="reference external" href="https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning">StarCraft</a>, and <a class="reference external" href="https://ai.facebook.com/blog/nethack-learning-environment-to-advance-deep-reinforcement-learning/">NetHack</a> rely on running multiple
environments (simulated games) in parallel using asynchronous
actor-critic methods. Straightforward multithreaded implementations
in Python don’t scale beyond more than a few parallel environments
due to GIL contention. Multiprocessing, with communication via
shared memory or UNIX sockets, adds much complexity and in effect
rules out interacting with CUDA from different workers, severely
restricting the design space.</div></blockquote>
<p>Manuel Kroiss, software engineer at DeepMind on the reinforcement
learning team, describes how the bottlenecks posed by the GIL lead to
rewriting Python codebases in C++, making the code less accessible:</p>
<blockquote>
<div>We frequently battle issues with the Python GIL at DeepMind. In many
of our applications, we would like to run on the order of 50-100
threads per process. However, we often see that even with fewer
than 10 threads the GIL becomes the bottleneck. To work around this
problem, we sometimes use subprocesses, but in many cases the
inter-process communication becomes too big of an overhead.  To
deal with the GIL, we usually end up translating large parts of our
Python codebase into C++. This is undesirable because it makes the
code less accessible to researchers.</div></blockquote>
<p>Projects that involve interfacing with multiple hardware devices face
similar challenges: efficient communication requires use of multiple
CPU cores.  The <a class="reference external" href="https://dose3d.fis.agh.edu.pl/en/projekt-dose-3d-z-programu-team-net-fnp-eng/">Dose-3D</a> project aims to improve cancer
radiotherapy with precise dose planning.  It uses medical phantoms
(stand-ins for human tissue) together with custom hardware and a
server application written in Python.  Paweł Jurgielewicz, lead
software architect for the data acquisition system on the Dose-3D
project, describes the scaling challenges posed by the GIL and how
using a fork of Python without the GIL simplified the project:</p>
<blockquote>
<div>In the Dose-3D project, the key challenge was to maintain a stable,
non-trivial concurrent communication link with hardware units while
utilizing a 1 Gbit/s UDP/IP connection to the maximum. Naturally,
we started with the multiprocessing package, but at some point, it
became clear that most CPU time was consumed by the data transfers
between the data processing stages, not by data processing itself.
The CPython multithreading implementation based on GIL was a dead
end too. When we found out about the “nogil” fork of Python it took
a single person less than half a working day to adjust the codebase
to use this fork and the results were astonishing. Now we can focus
on data acquisition system development rather than fine-tuning data
exchange algorithms.</div></blockquote>
<p>Allen Goodman, author of <a class="reference external" href="https://cellprofiler.org/">CellProfiler</a> and staff engineer at
Prescient Design and Genentech, describes how the GIL makes
biological methods research more difficult in Python:</p>
<blockquote>
<div>Issues with Python’s global interpreter lock are a frequent source
of frustration throughout biological methods research.<p>I wanted to better understand the current multithreading situation
so I reimplemented parts of <a class="reference external" href="http://hmmer.org/">HMMER</a>, a standard method for
multiple-sequence alignment. I chose this method because it
stresses both single-thread performance (scoring) and
multi-threaded performance (searching a database of sequences). The
GIL became the bottleneck when using only eight threads. This is a
method where the current popular implementations rely on 64 or
even 128 threads per process. I tried moving to subprocesses but
was blocked by the prohibitive IPC costs.  HMMER is a relatively
elementary bioinformatics method and newer methods have far bigger
multi-threading demands.</p>
<p>Method researchers are begging to use Python (myself included),
because of its ease of use, the Python ecosystem, and because “it’s
what people know.”  Many biologists only know a little bit of
programming (and that’s almost always Python). Until Python’s
multithreading situation is addressed, C and C++ will remain the
lingua franca of the biological methods research community.</p>
</div></blockquote>
</section>
<section id="the-gil-affects-python-library-usability">
<h3><a class="toc-backref" href="#the-gil-affects-python-library-usability" role="doc-backlink">The GIL Affects Python Library Usability</a></h3>
<p>The GIL is a CPython implementation detail that limits multithreaded
parallelism, so it might seem unintuitive to think of it as a
usability issue.  However, library authors frequently care a great
deal about performance and will design APIs that support working
around the GIL.  These workaround frequently lead to APIs that are
more difficult to use.  Consequently, users of these APIs may
experience the GIL as a <em>usability</em> issue and not just a performance
issue.</p>
<p>For example, PyTorch exposes a multiprocessing-based API called
<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> for building data input pipelines.  It uses <code class="docutils literal notranslate"><span class="pre">fork()</span></code>
on Linux because it is generally faster and uses less memory
than <code class="docutils literal notranslate"><span class="pre">spawn()</span></code>, but this leads to additional challenges for users:
creating a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> after accessing a GPU can lead to confusing
CUDA errors.  Accessing GPUs within a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> worker quickly
leads to out-of-memory errors because processes do not share CUDA
contexts (unlike threads within a process).</p>
<p>Olivier Grisel, scikit-learn developer and software engineer at Inria,
describes how having to work around the GIL in scikit-learn related
libraries leads to a more complex and confusing user experience:</p>
<blockquote>
<div>Over the years, scikit-learn developers have maintained ancillary
libraries such as <code class="docutils literal notranslate"><span class="pre">joblib</span></code> and <code class="docutils literal notranslate"><span class="pre">loky</span></code> to try to work around some
of the limitations of multiprocessing: extra memory usage partially
mitigated via semi-automated memory mapping of large data buffers,
slow worker startup by transparently reusing a pool of long
running workers, fork-safety problems of third-party native runtime
libraries such as GNU OpenMP by never using the fork-only
start-method, ability to perform parallel calls of interactively
defined functions in notebooks and REPLs in cross-platform manner
via cloudpickle. Despite our efforts, this multiprocessing-based
solution is still brittle, complex to maintain and confusing to
datascientists with limited understanding of system-level
constraints. Furthermore, there are still irreducible limitations
such as the overhead caused by the pickle-based
serialization/deserialization steps required for inter-process
communication. A lot of this extra work and complexity would not be
needed anymore if we could use threads without contention on
multicore hosts (sometimes with 64 physical cores or more) to run
data science pipelines that alternate between Python-level
operations and calls to native libraries.</div></blockquote>
<p>Ralf Gommers, co-director of Quansight Labs and NumPy and SciPy
maintainer, describes how the GIL affects the user experience of
NumPy and numeric Python libraries:</p>
<blockquote>
<div>A key problem in NumPy and the stack of packages built around it is
that NumPy is still (mostly) single-threaded — and that has shaped
significant parts of the user experience and projects built around
it. NumPy does release the GIL in its inner loops (which do the
heavy lifting), but that is not nearly enough. NumPy doesn’t offer
a solution to utilize all CPU cores of a single machine well, and
instead leaves that to Dask and other multiprocessing solutions.
Those aren’t very efficient and are also more clumsy to use. That
clumsiness comes mainly in the extra abstractions and layers the
users need to concern themselves with when using, e.g.,
<code class="docutils literal notranslate"><span class="pre">dask.array</span></code> which wraps <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>. It also shows up in
oversubscription issues that the user must explicitly be aware of
and manage via either environment variables or a third package,
<code class="docutils literal notranslate"><span class="pre">threadpoolctl</span></code>. The main reason is that NumPy calls into BLAS
for linear algebra - and those calls it has no control over, they
do use all cores by default via either pthreads or OpenMP.<p>Coordinating on APIs and design decisions to control parallelism is
still a major amount of work, and one of the harder challenges
across the PyData ecosystem. It would have looked a lot different
(better, easier) without a GIL.</p>
</div></blockquote>
</section>
<section id="gpu-heavy-workloads-require-multi-core-processing">
<h3><a class="toc-backref" href="#gpu-heavy-workloads-require-multi-core-processing" role="doc-backlink">GPU-Heavy Workloads Require Multi-Core Processing</a></h3>
<p>Many high-performance computing (HPC) and AI workloads make heavy use
of GPUs.  These applications frequently require efficient multi-core
CPU execution even though the bulk of the computation runs on a GPU.</p>
<p>Zachary DeVito, PyTorch core developer and researcher at FAIR
(Meta AI), describes how the GIL makes multithreaded scaling
inefficient even when the bulk of computation is performed outside of
Python:</p>
<blockquote>
<div>In PyTorch, Python is commonly used to orchestrate ~8 GPUs and ~64
CPU threads, growing to 4k GPUs and 32k CPU threads for big models.
While the heavy lifting is done outside of Python, the speed of
GPUs makes even just the orchestration in Python not scalable. We
often end up with 72 processes in place of one because of the GIL.
Logging, debugging, and performance tuning are orders-of-magnitude
more difficult in this regime, continuously causing lower developer
productivity.</div></blockquote>
<p>The use of many processes (instead of threads) makes common tasks more
difficult. Zachary DeVito continues:</p>
<blockquote>
<div>On three separate occasions in the past couple of months
(reducing redundant compute in data loaders, writing model
checkpoints asynchronously, and parallelizing compiler
optimizations), I spent an order-of-magnitude more time figuring
out how to work around GIL limitations than actually solving the
particular problem.</div></blockquote>
<p>Even GPU-heavy workloads frequently have a CPU-intensive component.
For example, computer vision tasks typically require
multiple “pre-processing” steps in the data input pipeline, like
image decoding, cropping, and resizing.  These tasks are commonly
performed on the CPU and may use Python libraries like <a class="reference external" href="https://pillow.readthedocs.io/en/stable/">Pillow</a>
or <a class="reference external" href="https://github.com/uploadcare/pillow-simd">Pillow-SIMD</a>.  It is necessary to run the data input pipeline
on multiple CPU cores in order to keep the GPU “fed” with data.</p>
<p>The increase in GPU performance compared to individual CPU cores makes
multi-core performance more important.  It is progressively more
difficult to keep the GPUs fully occupied.  To do so requires efficient
use of multiple CPU cores, especially on multi-GPU systems.  For
example, NVIDIA’s DGX-A100 has 8 GPUs and two 64-core CPUs in order to
keep the GPUs “fed” with data.</p>
</section>
<section id="the-gil-makes-deploying-python-ai-models-difficult">
<h3><a class="toc-backref" href="#the-gil-makes-deploying-python-ai-models-difficult" role="doc-backlink">The GIL Makes Deploying Python AI Models Difficult</a></h3>
<p>Python is widely used to develop neural network-based AI models.  In
PyTorch, models are frequently deployed as part of multi-threaded,
mostly C++, environments.  Python is often viewed skeptically
because the GIL can be a global bottleneck, preventing efficient
scaling even though the vast majority of the computations
occur “outside” of Python with the GIL released.  The torchdeploy
paper <a class="footnote-reference brackets" href="#torchdeploy" id="id2">[2]</a> shows experimental evidence for these scaling
bottlenecks in multiple model architectures.</p>
<p>PyTorch provides a number of mechanisms for deploying Python AI
models that avoid or work around the GIL, but they all come with
substantial limitations.  For example, <a class="reference external" href="https://pytorch.org/docs/stable/jit.html">TorchScript</a> captures a
representation of the model that can be executed from C++ without any
Python dependencies, but it only supports a limited subset of Python
and often requires rewriting some of the model’s code.  The
<a class="reference external" href="https://pytorch.org/docs/stable/package.html">torch::deploy</a> API
allows multiple Python interpreters, each with its own GIL, in the
same process(similar to <a class="pep reference internal" href="../pep-0684/" title="PEP 684 – A Per-Interpreter GIL">PEP 684</a>).  However, <code class="docutils literal notranslate"><span class="pre">torch::deploy</span></code> has
limited support for Python modules that use C-API extensions.</p>
</section>
<section id="motivation-summary">
<h3><a class="toc-backref" href="#motivation-summary" role="doc-backlink">Motivation Summary</a></h3>
<p>Python’s global interpreter lock makes it difficult to use modern
multi-core CPUs efficiently for many scientific and numeric computing
applications.  Heinrich Kuttler, Manuel Kroiss, and Paweł
Jurgielewicz found that multi-threaded implementations in Python did
not scale well for their tasks and that using multiple processes
was not a suitable alternative.</p>
<p>The scaling bottlenecks are not solely in core numeric tasks. Both
Zachary DeVito and Paweł Jurgielewicz described challenges with
coordination and communication in Python.</p>
<p>Olivier Grisel, Ralf Gommers, and Zachary DeVito described how current
workarounds for the GIL are “complex to maintain” and cause “lower
developer productivity.”  The GIL makes it more difficult to develop
and maintain scientific and numeric computing libraries as well
leading to library designs that are more difficult to use.</p>
</section>
</section>
<section id="specification">
<h2><a class="toc-backref" href="#specification" role="doc-backlink">Specification</a></h2>
<section id="build-configuration-changes">
<h3><a class="toc-backref" href="#build-configuration-changes" role="doc-backlink">Build Configuration Changes</a></h3>
<p>The global interpreter lock will remain the default for CPython builds
and python.org downloads. A new build configuration flag,
<code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> will be added to the configure script that will build
CPython with support for running without the global interpreter lock.</p>
<p>When built with <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code>, CPython will define the <code class="docutils literal notranslate"><span class="pre">Py_GIL_DISABLED</span></code>
macro in Python/patchlevel.h.  The ABI tag will include the letter “t”
(for “threading”).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> builds of CPython will still support optionally
running with the GIL enabled at runtime (see <a class="reference internal" href="#pythongil-environment-variable">PYTHONGIL Environment
Variable</a> and <a class="reference internal" href="#py-mod-gil-slot">Py_mod_gil Slot</a>).</p>
</section>
<section id="overview-of-cpython-changes">
<h3><a class="toc-backref" href="#overview-of-cpython-changes" role="doc-backlink">Overview of CPython Changes</a></h3>
<p>Removing the global interpreter lock requires substantial changes to
CPython internals, but relatively few changes to the public Python
and C APIs. This section describes the required changes to the
CPython implementation followed by the proposed API changes.</p>
<p>The implementation changes can be grouped into the following four
categories:</p>
<ul class="simple">
<li>Reference counting</li>
<li>Memory management</li>
<li>Container thread-safety</li>
<li>Locking and atomic APIs</li>
</ul>
</section>
<section id="reference-counting">
<h3><a class="toc-backref" href="#reference-counting" role="doc-backlink">Reference Counting</a></h3>
<p>Removing the GIL requires changes to CPython’s
reference counting implementation to make it thread-safe.
Furthermore, it needs to have low execution overhead and allow for
efficient scaling with multiple threads. This PEP proposes a
combination of three techniques to address these constraints. The
first is a switch from plain non-atomic reference counting to biased
reference counting, which is a thread-safe reference counting
technique with lower execution overhead than plain atomic reference
counting. The other two techniques are immortalization and a limited
form of deferred reference counting; they address some of the
multi-threaded scalability issues with reference counting by avoiding
some reference count modifications.</p>
<p>Biased reference counting (BRC) is a technique first described in 2018
by Jiho Choi, Thomas Shull, and Josep Torrellas <a class="footnote-reference brackets" href="#brc" id="id3">[3]</a>. It is based on the
observation that most objects are only accessed by a single thread,
even in multi-threaded programs. Each object is associated with an
owning thread (the thread that created it). Reference counting
operations from the owning thread use non-atomic instructions to
modify a “local” reference count. Other threads use atomic
instructions to modify a “shared” reference count. This design avoids
many atomic read-modify-write operations that are expensive on
contemporary processors.</p>
<p>The implementation of BRC proposed in this PEP largely matches the
original description of biased reference counting, but differs in
details like the size of reference counting fields and special bits in
those fields. BRC requires storing three pieces of information in each
object’s header: the “local” reference count, the “shared” reference
count, and the identifier of the owning thread.  The BRC paper packs
these three things into a single 64-bit field.  This PEP proposes using
three separate fields in each object’s header to avoid potential issues
due to reference count overflow.  Additionally, the PEP supports a
faster deallocation path that avoids an atomic operation in the common
case.</p>
<p>The proposed <code class="docutils literal notranslate"><span class="pre">PyObject</span></code> struct (also called <code class="docutils literal notranslate"><span class="pre">struct</span> <span class="pre">_object</span></code>) is
below:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">_object</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">_PyObject_HEAD_EXTRA</span>
<span class="w">  </span><span class="kt">uintptr_t</span><span class="w"> </span><span class="n">ob_tid</span><span class="p">;</span><span class="w">         </span><span class="c1">// owning thread id (4-8 bytes)</span>
<span class="w">  </span><span class="kt">uint16_t</span><span class="w"> </span><span class="n">__padding</span><span class="p">;</span><span class="w">       </span><span class="c1">// reserved for future use (2 bytes)</span>
<span class="w">  </span><span class="n">PyMutex</span><span class="w"> </span><span class="n">ob_mutex</span><span class="p">;</span><span class="w">         </span><span class="c1">// per-object mutex (1 byte)</span>
<span class="w">  </span><span class="kt">uint8_t</span><span class="w"> </span><span class="n">ob_gc_bits</span><span class="p">;</span><span class="w">       </span><span class="c1">// GC fields (1 byte)</span>
<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">ob_ref_local</span><span class="p">;</span><span class="w">    </span><span class="c1">// local reference count (4 bytes)</span>
<span class="w">  </span><span class="n">Py_ssize_t</span><span class="w"> </span><span class="n">ob_ref_shared</span><span class="p">;</span><span class="w"> </span><span class="c1">// shared reference count and state bits (4-8 bytes)</span>
<span class="w">  </span><span class="n">PyTypeObject</span><span class="w"> </span><span class="o">*</span><span class="n">ob_type</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">ob_tid</span></code>, <code class="docutils literal notranslate"><span class="pre">ob_ref_local</span></code>, and <code class="docutils literal notranslate"><span class="pre">ob_ref_shared</span></code> are used by
the biased reference counting implementation.  The <code class="docutils literal notranslate"><span class="pre">ob_gc_bits</span></code> field
is used store garbage collection flags that were previously stored in
<code class="docutils literal notranslate"><span class="pre">PyGC_Head</span></code> (see <a class="reference internal" href="#garbage-collection-cycle-collection">Garbage Collection (Cycle Collection)</a>).  The
<code class="docutils literal notranslate"><span class="pre">ob_mutex</span></code> field provides a per-object lock in a single byte.</p>
<section id="immortalization">
<h4><a class="toc-backref" href="#immortalization" role="doc-backlink">Immortalization</a></h4>
<p>Some objects, such as interned strings, small integers, statically
allocated PyTypeObjects, and the <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code>, and <code class="docutils literal notranslate"><span class="pre">None</span></code>
objects stay alive for the lifetime of the program. These objects are
marked as immortal by setting the local reference count field
(<code class="docutils literal notranslate"><span class="pre">ob_ref_local</span></code>) to <code class="docutils literal notranslate"><span class="pre">UINT32_MAX</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Py_INCREF</span></code> and <code class="docutils literal notranslate"><span class="pre">Py_DECREF</span></code> macros are no-ops for immortal
objects.  This avoids contention on the reference count fields of
these objects when multiple threads access them concurrently.</p>
<p>This proposed immortalization scheme is very similar to <a class="pep reference internal" href="../pep-0683/" title="PEP 683 – Immortal Objects, Using a Fixed Refcount">PEP 683</a>,
adopted in Python 3.12, but with slightly different bit representation
in the reference count fields for immortal objects in order to work
with biased reference counting and deferred reference counting.  See
also <a class="reference internal" href="#why-not-use-pep-683-immortalization">Why Not Use PEP 683 Immortalization?</a>.</p>
</section>
<section id="biased-reference-counting">
<h4><a class="toc-backref" href="#biased-reference-counting" role="doc-backlink">Biased Reference Counting</a></h4>
<p>Biased reference counting has a fast-path for objects “owned” by the
current thread and a slow-path for other objects.  Ownership is
indicated by the <code class="docutils literal notranslate"><span class="pre">ob_tid</span></code> field.  Determining the thread id requires
platform specific code <a class="footnote-reference brackets" href="#tid" id="id4">[5]</a>.  A value of <code class="docutils literal notranslate"><span class="pre">0</span></code> in <code class="docutils literal notranslate"><span class="pre">ob_tid</span></code>
indicates that the object is not owned by any thread.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ob_ref_local</span></code> field stores the local reference count and two
flags.  The two most significant bits are used to indicate the object
is immortal or uses deferred reference counting (see <a class="reference internal" href="#deferred-reference-counting">Deferred
reference counting</a>).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ob_ref_shared</span></code> field stores the shared reference count.  The
two <em>least</em> significant bits are used to store the reference
counting state.  The shared reference count is therefore shifted left by
two.  The <code class="docutils literal notranslate"><span class="pre">ob_ref_shared</span></code> field uses the least significant bits
because the shared reference count can be temporarily negative; increfs
and decrefs may not be balanced between threads.</p>
<p>The possible reference counting states are listed below:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">0b00</span></code> - default</li>
<li><code class="docutils literal notranslate"><span class="pre">0b01</span></code> - weakrefs</li>
<li><code class="docutils literal notranslate"><span class="pre">0b10</span></code> - queued</li>
<li><code class="docutils literal notranslate"><span class="pre">0b11</span></code> - merged</li>
</ul>
<p>The states form a progression: during their lifecycle, objects may
transition to any numerically higher state.  Objects can only be
deallocated from the “default” and “merged” states.  Other states must
transition to the “merged” state before deallocation.  Transitioning
states requires an atomic compare-and-swap on the <code class="docutils literal notranslate"><span class="pre">ob_ref_shared</span></code>
field.</p>
<section id="default-0b00">
<h5><a class="toc-backref" href="#default-0b00" role="doc-backlink">Default (<code class="docutils literal notranslate"><span class="pre">0b00</span></code>)</a></h5>
<p>Objects are initially created in the default state.  This is the only
state that allows for the quick deallocation code path.  Otherwise, the
thread must merge the local and shared reference count fields, which
requires an atomic compare-and-swap.</p>
<p>This quick deallocation code path would not be thread-safe with
concurrent dereferencing of weakrefs, so the first time a weak
reference is created, the object is transitioned to the “weakrefs”
state if it is currently in the “default” state.</p>
<p>Similarly, the quick deallocation code path would not be thread-safe
with the lockless list and dictionary accesses (see <a class="reference internal" href="#optimistically-avoiding-locking">Optimistically
Avoiding Locking</a>), so the first time a non-owning thread thread
attempts to retrieve an object in the “default” state it falls back to
the slower locking code path and transitions the object to
the “weakrefs” state.</p>
</section>
<section id="weakrefs-0b01">
<h5><a class="toc-backref" href="#weakrefs-0b01" role="doc-backlink">Weakrefs (<code class="docutils literal notranslate"><span class="pre">0b01</span></code>)</a></h5>
<p>Objects in weakref and higher states support dereferencing weakrefs
as well as the lockless list and dictionary access by non-owning
threads.  They require transitioning to the merged state before
deallocation, which is more expensive than the quick deallocation code
path supported by the “default” state.</p>
</section>
<section id="queued-0b10">
<h5><a class="toc-backref" href="#queued-0b10" role="doc-backlink">Queued (<code class="docutils literal notranslate"><span class="pre">0b10</span></code>)</a></h5>
<p>The queued state indicates that the a non-owning thread has requested
that the reference count fields be merged.  This can happen when the
shared reference count becomes negative (due to an imbalance between
increfs and decrefs between threads).  The object is inserted into the
owning thread’s queue of objects to be merged.  The owning thread is
notified via the <code class="docutils literal notranslate"><span class="pre">eval_breaker</span></code> mechanism.  In practice, this
operation is rare.  Most objects are only accessed by a single thread
and those objects accessed by multiple threads rarely have negative
shared reference counts.</p>
<p>If the owning thread has terminated, the acting thread immediately
merges the local and shared reference count fields and transitions to
the merged state.</p>
</section>
<section id="merged-0b11">
<h5><a class="toc-backref" href="#merged-0b11" role="doc-backlink">Merged (<code class="docutils literal notranslate"><span class="pre">0b11</span></code>)</a></h5>
<p>The merged state indicates that the object is not owned by any thread.
The <code class="docutils literal notranslate"><span class="pre">ob_tid</span></code> field is zero in this state and <code class="docutils literal notranslate"><span class="pre">ob_ref_local</span></code> is not
used.  Once the shared reference count reaches zero, the object can
be deallocated from the merged state.</p>
</section>
<section id="reference-counting-pseudo-code">
<h5><a class="toc-backref" href="#reference-counting-pseudo-code" role="doc-backlink">Reference counting pseudo-code</a></h5>
<p>The proposed <code class="docutils literal notranslate"><span class="pre">Py_INCREF</span></code> and <code class="docutils literal notranslate"><span class="pre">Py_DECREF</span></code> operation should behave
as follows (using C-like pseudo-code):</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="c1">// low two bits of &quot;ob_ref_shared&quot; are used for flags</span>
<span class="cp">#define _Py_SHARED_SHIFT 2</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">Py_INCREF</span><span class="p">(</span><span class="n">PyObject</span><span class="w"> </span><span class="o">*</span><span class="n">op</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">new_local</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_local</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">new_local</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="p">;</span><span class="w">  </span><span class="c1">// object is immortal</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_tid</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">_Py_ThreadId</span><span class="p">())</span>
<span class="w">    </span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_local</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_local</span><span class="p">;</span>
<span class="w">  </span><span class="k">else</span>
<span class="w">    </span><span class="n">atomic_add</span><span class="p">(</span><span class="o">&amp;</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_shared</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">_Py_SHARED_SHIFT</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">Py_DECREF</span><span class="p">(</span><span class="n">PyObject</span><span class="w"> </span><span class="o">*</span><span class="n">op</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_local</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">_Py_IMMORTAL_REFCNT</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="p">;</span><span class="w">  </span><span class="c1">// object is immortal</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_tid</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">_Py_ThreadId</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_local</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_local</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">_Py_MergeZeroRefcount</span><span class="p">();</span><span class="w"> </span><span class="c1">// merge refcount</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">_Py_DecRefShared</span><span class="p">();</span><span class="w"> </span><span class="c1">// slow path</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">_Py_MergeZeroRefcount</span><span class="p">(</span><span class="n">PyObject</span><span class="w"> </span><span class="o">*</span><span class="n">op</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_ref_shared</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// quick deallocation code path (common case)</span>
<span class="w">    </span><span class="n">op</span><span class="o">-&gt;</span><span class="n">ob_tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="n">_Py_Dealloc</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// slower merging path not shown</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The reference implementation <a class="footnote-reference brackets" href="#nogil312" id="id5">[17]</a> contains implementations of
<code class="docutils literal notranslate"><span class="pre">_Py_MergeZeroRefcount</span></code> and <code class="docutils literal notranslate"><span class="pre">_Py_DecRefShared</span></code>.</p>
<p>Note that the above is pseudocode: in practice, the implementation
should use “relaxed atomics” to access <code class="docutils literal notranslate"><span class="pre">ob_tid</span></code> and
<code class="docutils literal notranslate"><span class="pre">ob_ref_local</span></code> to avoid undefined behavior in C and C++.</p>
</section>
</section>
<section id="deferred-reference-counting">
<h4><a class="toc-backref" href="#deferred-reference-counting" role="doc-backlink">Deferred Reference Counting</a></h4>
<p>A few types of objects, such as top-level functions, code objects,
modules, and methods, tend to be frequently accessed by many threads
concurrently.  These objects don’t necessarily live for the lifetime of
the program, so immortalization is not a good fit. This PEP proposes a
limited form of deferred reference counting to avoid contention on
these objects’ reference count fields in multi-threaded programs.</p>
<p>Typically, the interpreter modifies objects’ reference counts as they
are pushed to and popped from the interpreter’s stack. The
interpreter skips these reference counting operations for objects
that use deferred reference counting.  Objects that support deferred
reference counting are marked by setting the two most significant
bits in the local reference count field to one.</p>
<p>Because some reference counting operations are skipped, the reference
count fields no longer reflect the true number of references to these
objects.  The true reference count is the sum of the reference count
fields plus any skipped references from each thread’s interpreter
stack.  The true reference count can only be safely computed when all
threads are paused during cyclic garbage collection.  Consequently,
objects that use deferred reference counting can only be deallocated
during garbage collection cycles.</p>
<p>Note that the objects that use deferred reference counting already
naturally form reference cycles in CPython, so they would typically be
deallocated by the garbage collector even without deferred reference
counting. For example, top-level functions and modules form a reference
cycle as do methods and type objects.</p>
</section>
<section id="garbage-collector-modifications-for-deferred-reference-counting">
<h4><a class="toc-backref" href="#garbage-collector-modifications-for-deferred-reference-counting" role="doc-backlink">Garbage Collector Modifications for Deferred Reference Counting</a></h4>
<p>The tracing garbage collector finds and deallocates unreferenced
objects.  Currently, the tracing garbage collector only finds
unreferenced objects that are part of a reference cycle. With
deferred reference counting, the tracing garbage collector will also
find and collect some unreferenced objects that may not be part of
any reference cycle, but whose collection has been delayed due to
deferred reference counting. This requires that all objects that
support deferred reference counting also have a corresponding type
object that supports tracing garbage collection (through the
<code class="docutils literal notranslate"><span class="pre">Py_TPFLAGS_HAVE_GC</span></code> flag). Additionally, the garbage collector
will need to traverse each thread’s stack to add references to the GC
reference count at the start of each collection.</p>
</section>
<section id="reference-counting-type-objects">
<h4><a class="toc-backref" href="#reference-counting-type-objects" role="doc-backlink">Reference Counting Type Objects</a></h4>
<p>Type objects (<code class="docutils literal notranslate"><span class="pre">PyTypeObject</span></code>) use a mix of reference counting
techniques. Statically allocated type objects are immortalized because
the objects already live for the lifetime of the program.  Heap type
objects use deferred reference counting in combination with per-thread
reference counting.  Deferred reference counting is not sufficient to
address the multi-threaded scaling bottlenecks with heap types because
most references to heap types are from object instances, not references
on the interpreter stack.</p>
<p>To address this, heap type reference counts are partially stored in a
distributed manner in per-thread arrays.  Every thread stores an
array of local reference counts for each heap type object.  Heap type
objects are assigned a unique number that determines its position in
the local reference count arrays.  A heap type’s true reference count
is the sum of its entries in the per-thread arrays, plus the reference
count on the <code class="docutils literal notranslate"><span class="pre">PyTypeObject</span></code>, plus any deferred references in the
interpreter stack.</p>
<p>Threads may grow their own type reference count arrays as needed when
incrementing or decrementing the local reference count of a type
object.</p>
<p>Use of the per-thread reference count arrays is limited to a few
places:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">PyType_GenericAlloc(PyTypeObject</span> <span class="pre">*type,</span> <span class="pre">Py_ssize_t</span> <span class="pre">nitems)</span></code>:
Increments the current thread’s local reference count for <code class="docutils literal notranslate"><span class="pre">type</span></code>,
if it is a heap type.</li>
<li><code class="docutils literal notranslate"><span class="pre">subtype_dealloc(PyObject</span> <span class="pre">*self)</span></code>: Decrements the current thread’s
local reference count for <code class="docutils literal notranslate"><span class="pre">self-&gt;ob_type</span></code>, if the type is a heap
type.</li>
<li><code class="docutils literal notranslate"><span class="pre">gcmodule.c</span></code>: Adds each thread’s local reference counts to the
<code class="docutils literal notranslate"><span class="pre">gc_refs</span></code> count for the corresponding heap type object.</li>
</ul>
<p>Additionally, when a thread terminates, it adds any non-zero local
reference counts to each type object’s own reference count field.</p>
</section>
</section>
<section id="memory-management">
<h3><a class="toc-backref" href="#memory-management" role="doc-backlink">Memory Management</a></h3>
<p>CPython currently uses an internal allocator, pymalloc, which is
optimized for small object allocation.  The pymalloc implementation is
not thread-safe without the GIL.  This PEP proposes replacing pymalloc
with mimalloc, a general-purpose thread-safe allocator with good
performance, including for small allocations.</p>
<p>Using mimalloc, with some modifications, also addresses two other
issues related to removing the GIL.  First, traversing the internal
mimalloc structures allows the garbage collector to find all Python
objects without maintaining a linked list.  This is described in more
detail in the garbage collection section.  Second, mimalloc heaps and
allocations based on size class enable collections like dict to
generally avoid acquiring locks during read-only operations. This is
described in more detail in the collection thread-safety section.</p>
<p>CPython already requires that objects that support garbage collection
use the GC allocator APIs (typically indirectly by calling
<code class="docutils literal notranslate"><span class="pre">PyType_GenericAlloc</span></code>). This PEP would add additional requirements
to the use of the Python allocator APIs. First, Python objects must
be allocated through object allocation APIs, such as
<code class="docutils literal notranslate"><span class="pre">PyType_GenericAlloc</span></code>, <code class="docutils literal notranslate"><span class="pre">PyObject_Malloc</span></code>, or other Python APIs
that wrap those calls. Python objects should not be allocated through
other APIs, such as raw calls to C’s malloc or the C++ new operator.
Additionally, <code class="docutils literal notranslate"><span class="pre">PyObject_Malloc</span></code> should be used only for allocating
Python objects; it should not be used for allocating buffers,
storages, or other data structures that are not PyObjects.</p>
<p>This PEP also imposes restrictions on the pluggable allocator API
(<code class="docutils literal notranslate"><span class="pre">PyMem_SetAllocator</span></code>). When compiling without the GIL, allocators
set using this API must eventually delegate the allocation to the
corresponding underlying allocator, such as <code class="docutils literal notranslate"><span class="pre">PyObject_Malloc</span></code>, for
Python object allocations. This allows for allocators that “wrap”
underlying allocators, such as Python’s tracemalloc and debug
allocator, but not for wholly replacing the allocator.</p>
<section id="cpython-free-lists">
<h4><a class="toc-backref" href="#cpython-free-lists" role="doc-backlink">CPython Free Lists</a></h4>
<p>CPython makes use of free lists to speed up the allocation of small,
frequently allocated objects like tuples and numbers.  These free
lists are moved to <code class="docutils literal notranslate"><span class="pre">PyThreadState</span></code> from per-interpreter state.</p>
</section>
</section>
<section id="garbage-collection-cycle-collection">
<h3><a class="toc-backref" href="#garbage-collection-cycle-collection" role="doc-backlink">Garbage Collection (Cycle Collection)</a></h3>
<p>The CPython garbage collector requires the following changes to work
with this proposal:</p>
<ul class="simple">
<li>Use of “stop-the-world” to provide thread-safety guarantees that
were previously provided by the GIL.</li>
<li>Elimination of generational garbage collection in favor of
non-generational collector.</li>
<li>Integration with deferred reference counting and biased reference
counting.</li>
</ul>
<p>Additionally, the above changes enable removing the
<code class="docutils literal notranslate"><span class="pre">_gc_prev</span></code> and <code class="docutils literal notranslate"><span class="pre">_gc_next</span></code> fields from GC objects.  The GC bits
that stored the tracked, finalized, and unreachable states are moved
to the <code class="docutils literal notranslate"><span class="pre">ob_gc_bits</span></code> field in the PyObject header.</p>
<section id="stop-the-world">
<h4><a class="toc-backref" href="#stop-the-world" role="doc-backlink">Stop-the-World</a></h4>
<p>The CPython cycle garbage collector currently relies on the global
interpreter lock to prevent other threads from accessing Python
objects while the collector finds cycles.  The GIL is never released
during the cycle-finding routine, so the collector can rely on
stable (i.e., unchanging) reference counts and references for the
duration of that routine. However, following cycle detection, the GIL
may be temporarily released while calling objects’ finalizers and
clear (<code class="docutils literal notranslate"><span class="pre">tp_clear</span></code>) functions, allowing other threads to run in an
interleaved fashion.</p>
<p>When running without the GIL, the implementation needs a way to ensure
that reference counts remain stable during cycle detection. Threads
running Python code must be paused to ensure that references and
reference counts remain stable. Once the cycles are identified, other
threads are resumed.</p>
<p>The current CPython cyclic garbage collector involves two
cycle-detection passes during each garbage collection cycle.
Consequently, this requires two stop-the-world pauses when running the
garbage collector without the GIL.  The first cycle-detection pass
identifies cyclic trash. The second pass runs after finalizers to
identify which objects still remain unreachable.  Note that other
threads are resumed before finalizers and <code class="docutils literal notranslate"><span class="pre">tp_clear</span></code> functions are
called to avoid introducing potential deadlocks that are not present in
the current CPython behavior.</p>
</section>
<section id="thread-states">
<h4><a class="toc-backref" href="#thread-states" role="doc-backlink">Thread States</a></h4>
<p>To support pausing threads for garbage collection, the PyThreadState
gets a new “status” field. Like the other fields in PyThreadState,
the status field is not part of the public CPython API. The status
field may be in one of three states:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">ATTACHED</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">DETACHED</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">GC</span></code></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">ATTACHED</span></code> and <code class="docutils literal notranslate"><span class="pre">DETACHED</span></code> states correspond closely to
acquiring and releasing the global interpreter lock. When compiling
without the GIL, functions that previously acquired the GIL instead
transition the thread state to <code class="docutils literal notranslate"><span class="pre">ATTACHED</span></code>, and functions that
previously released the GIL transition the thread state
to <code class="docutils literal notranslate"><span class="pre">DETACHED</span></code>. Just as threads previously needed to acquire the
GIL before accessing or modifying Python objects, they now must be in
the <code class="docutils literal notranslate"><span class="pre">ATTACHED</span></code> state before accessing or modifying Python
objects. Since the same public C-API functions “attach” the thread as
previously acquired the GIL (e.g., <code class="docutils literal notranslate"><span class="pre">PyEval_RestoreThread</span></code>), the
requirements for thread initialization in extensions remain the same.
The substantial difference is that multiple threads can be in the
attached state simultaneously, while previously only one thread could
acquire the GIL at a time.</p>
<p>During stop-the-world pauses, the thread performing garbage collection
needs to ensure that no other thread is accessing or modifying Python
objects.  All other threads must be in the “GC” state. The garbage
collection thread can transition other threads from the <code class="docutils literal notranslate"><span class="pre">DETACHED</span></code>
state to the GC state using an atomic compare-and-swap operation on
the status field. Threads in the <code class="docutils literal notranslate"><span class="pre">ATTACHED</span></code> state are requested to
pause themselves and set their status to “GC”, using the
existing “eval breaker” mechanism. At the end of the stop-the-world
pause, all threads in the “GC” state are set to <code class="docutils literal notranslate"><span class="pre">DETACHED</span></code> and
woken up if they are paused. Threads that were previously attached
(i.e., executing Python bytecode) can re-attach (set their thread
states to <code class="docutils literal notranslate"><span class="pre">ATTACHED</span></code>) and resume executing Python code. Threads
that were previously <code class="docutils literal notranslate"><span class="pre">DETACHED</span></code> ignore the notification.</p>
</section>
<section id="generations">
<h4><a class="toc-backref" href="#generations" role="doc-backlink">Generations</a></h4>
<p>The existing Python garbage collector uses three generations.  When
compiling without the GIL, the garbage collector will only use a single
generation (i.e., it will be non-generational).  The primary reason for
this change is to reduce the impact of the stop-the-world pauses in
multithreaded applications.  Frequent stop-the-world pauses for
collecting the young generation would have more of an impact on
multi-threaded applications than less frequent collections.</p>
</section>
<section id="integration-with-deferred-and-biased-reference-counting">
<h4><a class="toc-backref" href="#integration-with-deferred-and-biased-reference-counting" role="doc-backlink">Integration With Deferred and Biased Reference Counting</a></h4>
<p>To find unreferenced objects, the cyclic garbage collector computes
the difference between the number of incoming references and the
object’s reference count.  This difference is called <code class="docutils literal notranslate"><span class="pre">gc_refs</span></code> and
is stored in the <code class="docutils literal notranslate"><span class="pre">_gc_prev</span></code> field.  If <code class="docutils literal notranslate"><span class="pre">gc_refs</span></code> is greater than
zero, then the object is guaranteed to be alive (i.e., not cyclic
trash). If <code class="docutils literal notranslate"><span class="pre">gc_refs</span></code> is zero, then the object is only alive if it
is transitively referenced by another live object. When computing
this difference, the collector should traverse each thread’s stack,
and for every deferred reference, increment the <code class="docutils literal notranslate"><span class="pre">gc_refs</span></code> for the
referred object. Since generator objects also have stacks with
deferred references, the same procedure is applied to each
generator’s stack.</p>
<p>Python unit tests commonly use <code class="docutils literal notranslate"><span class="pre">gc.collect()</span></code> to ensure that any
unreferenced objects are destructed and their finalizers run.  Since
biased reference counting can delay the destruction of some objects
that are referenced by multiple threads, it’s convenient to ensure
that those objects are destructed during garbage collection, even
though they may not be part of any reference cycles.  While other
threads are paused, the garbage collector thread should merge the
reference counts for any queued objects, but not call any destructors
even if the combined reference count is zero. (Calling destructors
while other threads are paused risks introducing deadlocks.) Once
other threads are resumed, the GC thread should call <code class="docutils literal notranslate"><span class="pre">_Py_Dealloc</span></code>
on those objects with a zero merged reference count.</p>
</section>
</section>
<section id="container-thread-safety">
<h3><a class="toc-backref" href="#container-thread-safety" role="doc-backlink">Container Thread-Safety</a></h3>
<p>In CPython, the global interpreter lock protects against corruption of
internal interpreter states when multiple threads concurrently access
or modify Python objects.  For example, if multiple threads
concurrently modify the same list, the GIL ensures that the length of
the list (<code class="docutils literal notranslate"><span class="pre">ob_size</span></code>) accurately matches the number of elements, and
that the reference counts of each element accurately reflect the
number of references to those elements. Without the GIL — and
absent other changes — concurrent modifications would corrupt those
fields and likely lead to program crashes.</p>
<p>The GIL does not necessarily ensure that operations are atomic or
remain correct when multiple operations occur concurrently. For
example, <code class="docutils literal notranslate"><span class="pre">list.extend(iterable)</span></code> may not appear atomic if the
iterable has an iterator implemented in Python (or releases the GIL
internally). Similarly, <code class="docutils literal notranslate"><span class="pre">list.remove(x)</span></code> can remove the wrong
object if it overlaps with another operation that modifies the list,
depending on the implementation of the equality operator.  Still, the
GIL ensures that some operations are effectively atomic. For example,
the constructor <code class="docutils literal notranslate"><span class="pre">list(set)</span></code> atomically copies the items of the set
to a new list, and some code relies on that copy being atomic
(i.e., having a snapshot of the items in the set). This PEP preserves
that property.</p>
<p>This PEP proposes using per-object locks to provide many of the same
protections that the GIL provides.  For example, every list,
dictionary, and set will have an associated lightweight lock.  All
operations that modify the object must hold the object’s lock.  Most
operations that read from the object should acquire the object’s lock
as well; the few read operations that can proceed without holding a
lock are described below.</p>
<p>Per-object locks with critical sections provide weaker protections
than the GIL. Because the GIL doesn’t necessarily ensure that
concurrent operations are atomic or correct, the per-object locking
scheme also cannot ensure that concurrent operations are atomic or
correct. Instead, per-object locking aims for similar protections as
the GIL, but with mutual exclusion limited to individual objects.</p>
<p>Most operations on an instance of a container type require locking
that object. For example:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">list.append</span></code>, <code class="docutils literal notranslate"><span class="pre">list.insert</span></code>, <code class="docutils literal notranslate"><span class="pre">list.repeat</span></code>,
<code class="docutils literal notranslate"><span class="pre">PyList_SetItem</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">dict.__setitem__</span></code>, <code class="docutils literal notranslate"><span class="pre">PyDict_SetItem</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">list.clear</span></code>, <code class="docutils literal notranslate"><span class="pre">dict.clear</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">list.__repr__</span></code>, <code class="docutils literal notranslate"><span class="pre">dict.__repr__</span></code>, etc.</li>
<li><code class="docutils literal notranslate"><span class="pre">list.extend(iterable)</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">setiter_iternext</span></code></li>
</ul>
<p>Some operations operate directly on two container objects, with
knowledge about both containers’ internal structure.  For example,
there are internal specializations of <code class="docutils literal notranslate"><span class="pre">list.extend(iterable)</span></code> for
specific iterable types, like <code class="docutils literal notranslate"><span class="pre">set</span></code>. These operations need to lock
both container objects because they access the internals of both
objects simultaneously.  Note that the generic implementation of
<code class="docutils literal notranslate"><span class="pre">list.extend</span></code> only needs to lock one object (the list) because the
other object is accessed indirectly through the thread-safe iterator
API.  Operations that lock two containers are:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">list.extend(list)</span></code>, <code class="docutils literal notranslate"><span class="pre">list.extend(set)</span></code>, <code class="docutils literal notranslate"><span class="pre">list.extend</span>
<span class="pre">(dictitems)</span></code>, and other specializations where the implementation
is specialized for argument type.</li>
<li><code class="docutils literal notranslate"><span class="pre">list.concat(list)</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">list.__eq__(list)</span></code>, <code class="docutils literal notranslate"><span class="pre">dict.__eq__(dict)</span></code></li>
</ul>
<p>Some simple operations can be implemented directly with atomic
accesses and do not need locks because they only access a single
field.  These operations include:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">len(list)</span></code> i.e., <code class="docutils literal notranslate"><span class="pre">list_length(PyListObject</span> <span class="pre">*a)</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">len(dict)</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">len(set)</span></code></li>
</ul>
<p>A select few operations optimistically avoid locking to improve
performance. These require special implementations and cooperation
from the memory allocator:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">list[idx]</span></code> (<code class="docutils literal notranslate"><span class="pre">list_subscript</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">dict[key]</span></code> (<code class="docutils literal notranslate"><span class="pre">dict_subscript</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">listiter_next</span></code>, <code class="docutils literal notranslate"><span class="pre">dictiter_iternextkey/value/item</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">list.contains</span></code></li>
</ul>
<section id="borrowed-references">
<h4><a class="toc-backref" href="#borrowed-references" role="doc-backlink">Borrowed References</a></h4>
<p>Per-object locking provides many of the important protections that the
GIL provides, but there are a few cases where it’s not sufficient.
For example, code that relies on upgrading a borrowed reference to
an “owned” reference may be unsafe in certain circumstances:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">PyObject</span><span class="w"> </span><span class="o">*</span><span class="n">item</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">PyList_GetItem</span><span class="p">(</span><span class="n">list</span><span class="p">,</span><span class="w"> </span><span class="n">idx</span><span class="p">);</span>
<span class="n">Py_INCREF</span><span class="p">(</span><span class="n">item</span><span class="p">);</span>
</pre></div>
</div>
<p>The GIL ensures that no other thread can modify the list in between
the access and the <code class="docutils literal notranslate"><span class="pre">Py_INCREF</span></code> call. Without the GIL – even with
per-object locking – another thread might modify the list leading to
<code class="docutils literal notranslate"><span class="pre">item</span></code> being freed between the access and the <code class="docutils literal notranslate"><span class="pre">Py_INCREF</span></code> call.</p>
<p>The problematic borrowed reference APIs are supplemented with
functions that return “new references” but are otherwise
equivalent:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">PyList_FetchItem(list,</span> <span class="pre">idx)</span></code> for <code class="docutils literal notranslate"><span class="pre">PyList_GetItem</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">PyDict_FetchItem(dict,</span> <span class="pre">key)</span></code> for <code class="docutils literal notranslate"><span class="pre">PyDict_GetItem</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">PyWeakref_FetchObject</span></code> for <code class="docutils literal notranslate"><span class="pre">PyWeakref_GetObject</span></code></li>
</ul>
<p>Note that some APIs that return borrowed references, such as
<code class="docutils literal notranslate"><span class="pre">PyTuple_GetItem</span></code>, are not problematic because tuples are
immutable. Similarly, not all uses of the above APIs are problematic.
For example, <code class="docutils literal notranslate"><span class="pre">PyDict_GetItem</span></code> is often used for parsing keyword
argument dictionaries in function calls; those keyword argument
dictionaries are effectively private (not accessible by other
threads).</p>
</section>
<section id="python-critical-sections">
<h4><a class="toc-backref" href="#python-critical-sections" role="doc-backlink">Python Critical Sections</a></h4>
<p>Straightforward per-object locking could introduce deadlocks that were
not present when running with the GIL.  Threads may hold locks for
multiple objects simultaneously because Python operations can nest.
Operations on objects can invoke operations on other objects,
acquiring multiple per-object locks.  If threads try to acquire the
same locks in different orders, they will deadlock.</p>
<p>This PEP proposes a scheme called “Python critical sections” to
implicitly release per-object locks to avoid deadlocks.  To
understand the scheme, we first introduce a general approach to avoid
deadlocks, and then propose a refinement of that approach with better
performance.</p>
<p>One way to avoid deadlocks is to allow threads to hold only the lock
(or locks) for a single operation at a time (typically a single lock,
but some operations involve two locks as described above).  When a
thread begins a nested operation it should suspend the locks for any
outer operation: before beginning the nested operation, the locks for
the outer operation are released and when the nested operation
completes, the locks for the outer operation are reacquired.</p>
<p>Additionally, the locks for any active operation should be suspended
around potentially blocking operations, such as I/O (i.e., operations
that would have released the GIL). This is because the interaction
between locks and blocking operations can lead to deadlocks in the
same way as the interaction between multiple locks.</p>
<p>To improve performance, this PEP proposes a variation of the above
scheme that still avoids deadlocks.  Instead of immediately
suspending locks any time a nested operation begins, locks are only
suspended if the thread would block (i.e., would have released the
GIL).  This reduces the number of lock acquisitions and releases for
nested operations, while avoiding deadlocks.</p>
<p>The proposed API for Python critical sections are the following four
macros. These are intended to be public (usable by C-API extensions),
but not part of the limited API:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">Py_BEGIN_CRITICAL_SECTION(PyObject</span> <span class="pre">*op);</span></code>:
Begins a critical section by acquiring the mutex for the referenced
object.  If the object is  already locked, then locks for any
outstanding critical sections are released before this thread waits
for referenced object to be unlocked.</li>
<li><code class="docutils literal notranslate"><span class="pre">Py_END_CRITICAL_SECTION;</span></code>:
Ends the most recent operation, unlocking the mutex. The next
most recent previous critical section (if any) is resumed if it is
currently suspended.</li>
<li><code class="docutils literal notranslate"><span class="pre">Py_BEGIN_CRITICAL_SECTION2(PyObject</span> <span class="pre">*a,</span> <span class="pre">PyObject</span> <span class="pre">*b);</span></code>:
Begins a critical section by acquiring the mutexes for two objects.
To ensure consistent lock ordering, the order of acquisition is
determined by memory address (i.e., the mutex with lower memory
address is acquired first). If either mutex is already locked, then
locks for any outstanding critical sections are released before this
thread waits for the referenced objects to be unlocked.</li>
<li><code class="docutils literal notranslate"><span class="pre">Py_END_CRITICAL_SECTION2;</span></code>:
Behaves the same as <code class="docutils literal notranslate"><span class="pre">Py_END_CRITICAL_SECTION</span></code> but unlocks two
objects.</li>
</ul>
<p>Additionally, when a thread transitions from the <code class="docutils literal notranslate"><span class="pre">ATTACHED</span></code> state to
the <code class="docutils literal notranslate"><span class="pre">DETACHED</span></code> state, it should suspend any active critical
sections. When transitioning from <code class="docutils literal notranslate"><span class="pre">DETACHED</span></code> to <code class="docutils literal notranslate"><span class="pre">ATTACHED</span></code>, the
most recent suspended critical section, if any, should be resumed.</p>
<p>Note that operations that lock two containers simultaneously need to use
the <code class="docutils literal notranslate"><span class="pre">Py_BEGIN_CRITICAL_SECTION2</span></code> macro.  It is not sufficient to nest
two calls to <code class="docutils literal notranslate"><span class="pre">Py_BEGIN_CRITICAL_SECTION</span></code> because the inner critical
section may release the locks from the outer critical section.</p>
</section>
<section id="optimistically-avoiding-locking">
<h4><a class="toc-backref" href="#optimistically-avoiding-locking" role="doc-backlink">Optimistically Avoiding Locking</a></h4>
<p>A few operations on <code class="docutils literal notranslate"><span class="pre">dict</span></code> and <code class="docutils literal notranslate"><span class="pre">list</span></code> optimistically avoid
acquiring the per-object locks. They have a fast path operation that
does not acquire locks, but may fall back to a slower operation that
acquires the dictionary’s or list’s lock when another thread is
concurrently modifying that container.</p>
<p>The operations with an optimistic fast path are:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">PyDict_FetchItem/GetItem</span></code> and <code class="docutils literal notranslate"><span class="pre">dict.__getitem__</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">PyList_FetchItem/GetItem</span></code> and <code class="docutils literal notranslate"><span class="pre">list.__getitem__</span></code></li>
</ul>
<p>Additionally, iterators for <code class="docutils literal notranslate"><span class="pre">dict</span></code> and <code class="docutils literal notranslate"><span class="pre">list</span></code> use the above
functions so they also optimistically avoid locking when returning
the next item.</p>
<p>There are two motivations for avoiding lock acquisitions in these
functions. The primary reason is that it is necessary for scalable
multi-threaded performance even for simple applications. Dictionaries
hold top-level functions in modules and methods for classes. These
dictionaries are inherently highly shared by many threads in
multi-threaded programs. Contention on these locks in multi-threaded
programs for loading methods and functions would inhibit efficient
scaling in many basic programs.</p>
<p>The secondary motivation for avoiding locking is to reduce overhead
and improve single-threaded performance.  Although lock acquisition
has low overhead compared to most operations, accessing individual
elements of lists and dictionaries are fast operations (so the
locking overhead is comparatively larger) and frequent (so the
overhead has more impact).</p>
<p>This section describes the challenges with implementing dictionary and
list accesses without locking followed by a description of this PEP’s
changes to the Python interpreter required to address those
challenges.</p>
<p>The main challenge is that retrieving an item from a list or
dictionary and incrementing the reference count of that item is not
an atomic operation. In between the time the item is retrieved and
the reference count is incremented, another thread may modify the
list or dictionary, possibly freeing the memory for the previously
retrieved item.</p>
<p>A partial attempt at addressing this issue would be to convert the
reference count increment to a conditional increment, only
incrementing the reference count if it’s not zero.  This change is
not sufficient because when a Python object’s reference count reaches
zero, the object’s destructor is called and the memory storing the
object may be re-used for other data structures or returned to the
operating system.  Instead, this PEP proposes a technique to ensure
that the reference count fields remain valid for the duration of the
access, so that the conditional reference count increment is safe.
This technique requires cooperation from the memory allocator
(mimalloc) as well as changes to the list and dictionary objects. The
proposed technique is similar to read-copy update (RCU) <a class="footnote-reference brackets" href="#rcu" id="id6">[6]</a>, a
synchronization mechanism widely used in the Linux kernel.</p>
<p>The current implementation of <code class="docutils literal notranslate"><span class="pre">list_item</span></code> (the C function
implementing <code class="docutils literal notranslate"><span class="pre">list.__getitem__</span></code>) is the following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Py_INCREF</span><span class="p">(</span><span class="n">a</span><span class="o">-&gt;</span><span class="n">ob_item</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="k">return</span><span class="w"> </span><span class="n">a</span><span class="o">-&gt;</span><span class="n">ob_item</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</pre></div>
</div>
<p>The proposed implementation uses the conditional increment
(<code class="docutils literal notranslate"><span class="pre">_Py_TRY_INCREF</span></code>) and has additional checks:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="n">PyObject</span><span class="w"> </span><span class="o">**</span><span class="n">ob_item</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">atomic_load</span><span class="p">(</span><span class="o">&amp;</span><span class="n">a</span><span class="o">-&gt;</span><span class="n">ob_item</span><span class="p">);</span>
<span class="w"> </span><span class="n">PyObject</span><span class="w"> </span><span class="o">*</span><span class="n">item</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">atomic_load</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ob_item</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">item</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">!</span><span class="n">_Py_TRY_INCREF</span><span class="p">(</span><span class="n">item</span><span class="p">))</span><span class="w"> </span><span class="k">goto</span><span class="w"> </span><span class="n">retry</span><span class="p">;</span>
<span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">item</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">atomic_load</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ob_item</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="n">Py_DECREF</span><span class="p">(</span><span class="n">item</span><span class="p">);</span>
<span class="w">   </span><span class="k">goto</span><span class="w"> </span><span class="n">retry</span><span class="p">;</span>
<span class="w"> </span><span class="p">}</span>
<span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ob_item</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">atomic_load</span><span class="p">(</span><span class="o">&amp;</span><span class="n">a</span><span class="o">-&gt;</span><span class="n">ob_item</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="n">Py_DECREF</span><span class="p">(</span><span class="n">item</span><span class="p">);</span>
<span class="w">   </span><span class="k">goto</span><span class="w"> </span><span class="n">retry</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">return</span><span class="w"> </span><span class="n">item</span><span class="p">;</span>
</pre></div>
</div>
<p>The “retry” subroutine implements the locked fallback path when
concurrent modifications to the list cause the above fast,
non-locking path to fail:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="nl">retry</span><span class="p">:</span>
<span class="w">  </span><span class="n">PyObject</span><span class="w"> </span><span class="o">*</span><span class="n">item</span><span class="p">;</span>
<span class="w">  </span><span class="n">Py_BEGIN_CRITICAL_SECTION</span><span class="p">(</span><span class="n">a</span><span class="o">-&gt;</span><span class="n">ob_mutex</span><span class="p">);</span>
<span class="w">  </span><span class="n">item</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="o">-&gt;</span><span class="n">ob_item</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="n">Py_INCREF</span><span class="p">(</span><span class="n">item</span><span class="p">);</span>
<span class="w">  </span><span class="n">Py_END_CRITICAL_SECTION</span><span class="p">(</span><span class="n">a</span><span class="o">-&gt;</span><span class="n">ob_mutex</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">item</span><span class="p">;</span>
</pre></div>
</div>
<p>The modifications to the <code class="docutils literal notranslate"><span class="pre">dict</span></code> implementation are similar, because
the relevant parts of both list and dictionary retrieval involve
loading an item/value from an array at a known index.</p>
<p>The additional checks following the conditional increment are
necessary because the scheme allows immediate re-use of memory,
including the memory that previously held a <code class="docutils literal notranslate"><span class="pre">PyObject</span></code> structure or
<code class="docutils literal notranslate"><span class="pre">list</span></code> or <code class="docutils literal notranslate"><span class="pre">dict</span></code> array.  Without these extra checks, the function
might return a Python object that was never in the list, if the
memory occupied by the Python object previously held a different
<code class="docutils literal notranslate"><span class="pre">PyObject</span></code> whose memory previously stored an item in the list.</p>
</section>
<section id="mimalloc-changes-for-optimistic-list-and-dict-access">
<h4><a class="toc-backref" href="#mimalloc-changes-for-optimistic-list-and-dict-access" role="doc-backlink">Mimalloc Changes for Optimistic <code class="docutils literal notranslate"><span class="pre">list</span></code> and <code class="docutils literal notranslate"><span class="pre">dict</span></code> Access</a></h4>
<p>The implementation requires additional constraints to the memory
allocator, including some changes to the mimalloc code.  Some
background on mimalloc’s implementation is helpful to understand the
required changes.  Individual allocations from mimalloc are
called “blocks.”  Mimalloc “pages” contain consecutive blocks that
are all the same size.  A mimalloc “page” is similar to
a “superblock” in other allocators; it is NOT an operating system
page.  A mimalloc “heap” contains pages of various size classes; each
page belongs to a single heap. If none of the blocks of a page are
allocated, then mimalloc may re-use the page for a different size
class or different heap (i.e., it might reinitialize the page).</p>
<p>The list and dictionary access scheme works by partially restricting
re-use of mimalloc pages so that reference count fields remains valid
for the duration of the access.  The restricted re-use of mimalloc
pages is enforced by having separate heaps for Python objects
<a class="footnote-reference brackets" href="#heaps" id="id7">[7]</a>.  This ensures that even if an item is freed during access
and the memory reused for a new object, the new object’s reference
count field is placed at the same location in memory.  The reference
count field remains valid (or zero) across allocations.</p>
<p>Python objects that support <code class="docutils literal notranslate"><span class="pre">Py_TPFLAGS_MANAGED_DICT</span></code> have their
dictionary and weak reference fields preceding the  <code class="docutils literal notranslate"><span class="pre">PyObject</span></code>
header, so their reference count fields are at a different offset from
the start of their allocations.  They are stored in a separate mimalloc
heap.  Additionally, non-GC objects are stored in their own heap so
that the GC only has to look at GC objects.  There are therefore three
mimalloc heaps for Python objects, one for non-GC objects, one for GC
objects with managed dictionaries, and one for GC objects without
managed dictionaries.</p>
</section>
<section id="mimalloc-page-reuse">
<h4><a class="toc-backref" href="#mimalloc-page-reuse" role="doc-backlink">Mimalloc Page Reuse</a></h4>
<p>It is beneficial to keep the restrictions on mimalloc page reuse to a
short period of time to avoid increasing overall memory usage.
Precisely limiting the restrictions to list and dictionary accesses
would minimize memory usage, but would require expensive
synchronizations.  At the other extreme, keeping the restrictions
until the next GC cycle would avoid introducing any extra
synchronizations, but would potentially increase memory usage.</p>
<p>This PEP proposes a system that lies between those two extremes based
on FreeBSD’s “GUS” <a class="footnote-reference brackets" href="#gus" id="id8">[8]</a>.  It uses a combination of global and
per-thread counters (or “sequence numbers”) to coordinate the
determination of when it is safe to reuse an empty mimalloc page for
a different heap or for a different size class, or to return it to
the operating system:</p>
<ul class="simple">
<li>There is a global write sequence number that monotonically
increases.</li>
<li>When a mimalloc page is empty, it’s tagged with the current write
sequence number.  The thread may also atomically increment the
global write sequence number.</li>
<li>Each thread has a local read sequence number that records the most
recent write sequence number it has observed.</li>
<li>Threads may observe the write sequence number whenever they are not
in a list or dictionary access.  The reference implementation does
this in mimalloc’s slow-path allocation function.  This is called
regularly enough to be useful, but not so frequently as to
introduce significant overhead.</li>
<li>There is a global read sequence number that stores the minimum of
all active threads’ read sequence numbers.  A thread may update the
global read sequence number by scanning each threads’ local read
sequence number.  The reference implementation does this before
allocating a fresh mimalloc page if there are restricted pages
that could possibly be reused.</li>
<li>An empty mimalloc page may be reused for a different heap or size
class when the global read sequence number is larger than the
page’s tag number.</li>
</ul>
<p>The condition that the global read sequence number is larger than the
page’s tag is sufficient because it ensures that any thread that had
a concurrent optimistic list or dictionary access is finished with
that access.  In other words, there are no threads accessing the
empty blocks in the freed page, so the page can be used for any other
purpose or even returned to the operating system.</p>
</section>
<section id="optimistic-dict-and-list-access-summary">
<h4><a class="toc-backref" href="#optimistic-dict-and-list-access-summary" role="doc-backlink">Optimistic <code class="docutils literal notranslate"><span class="pre">dict</span></code> and <code class="docutils literal notranslate"><span class="pre">list</span></code> Access Summary</a></h4>
<p>This PEP proposes a technique for thread-safe list and dictionary
accesses that typically avoids acquiring locks.  This reduces
execution overhead and avoids some multi-threaded scaling bottlenecks
in common operations, like calling functions and methods.  The scheme
works by placing temporary restrictions on mimalloc page reuse to
ensure that objects’ reference count fields remain valid after
objects are freed so that conditional reference count increment
operations are safe.  The restrictions are placed on mimalloc pages
instead of on individual objects to improve opportunities for memory
reuse.  The restrictions are lifted as soon as the system can
determine that there are no outstanding accesses involving the empty
mimalloc page.  To determine this, the system uses a combination of
lightweight per-thread sequence counters and also tags pages when
they are empty.  Once each thread’s local counter is larger than the
page’s tag, it can be reused for any purpose or returned to the
operating system.  The restrictions are also lifted whenever the
cyclic garbage collector runs because the stop-the-world pause
ensures that threads do not have any outstanding references to empty
mimalloc pages.</p>
</section>
</section>
<section id="specializing-interpreter">
<h3><a class="toc-backref" href="#specializing-interpreter" role="doc-backlink">Specializing Interpreter</a></h3>
<p>The specializing interpreter requires some changes to be thread-safe
when running without the GIL:</p>
<ul class="simple">
<li>Concurrent specializations are prevented by using a mutex.  This
prevents multiple threads writing to the same inline cache.</li>
<li>In multi-threaded programs running without the GIL, each bytecode is
only specialized once.  This prevents a thread from reading a
partially written inline cache.</li>
<li>Locking also ensures that cached values of <code class="docutils literal notranslate"><span class="pre">tp_version_tag</span></code> and
<code class="docutils literal notranslate"><span class="pre">keys_version</span></code> are consistent with the cached descriptors and other
values.</li>
<li>Modifications to inline counters use “relaxed atomics”.  In other
words, some counter decrements may be missed or overwritten, but that
does not affect correctness.</li>
</ul>
</section>
<section id="py-mod-gil-slot">
<h3><a class="toc-backref" href="#py-mod-gil-slot" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">Py_mod_gil</span></code> Slot</a></h3>
<p>In <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> builds, when loading an extension, CPython will
check for a new <a class="pep reference internal" href="../pep-0489/" title="PEP 489 – Multi-phase extension module initialization">PEP 489</a>-style <code class="docutils literal notranslate"><span class="pre">Py_mod_gil</span></code> slot.  If the slot is
set to <code class="docutils literal notranslate"><span class="pre">Py_mod_gil_not_used</span></code>, then extension loading proceeds as
normal. If the slot is not set, the interpreter pauses all threads and
enables the GIL before continuing.  Additionally, the interpreter will
issue a visible warning naming the extension, that the GIL was enabled
(and why) and the steps the user can take to override it.</p>
</section>
<section id="pythongil-environment-variable">
<h3><a class="toc-backref" href="#pythongil-environment-variable" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">PYTHONGIL</span></code> Environment Variable</a></h3>
<p>In <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> builds, the user can also override the behavior at
runtime by setting the <code class="docutils literal notranslate"><span class="pre">PYTHONGIL</span></code> environment variable. Setting
<code class="docutils literal notranslate"><span class="pre">PYTHONGIL=0</span></code>, forces the GIL to be disabled, overriding the module
slot logic.  Setting <code class="docutils literal notranslate"><span class="pre">PYTHONGIL=1</span></code>, forces the GIL to be enabled.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">PYTHONGIL=0</span></code> override is important because extensions that are
not thread-safe can still be useful in multi-threaded applications. For
example, one may want to use the extension from only a single thread or
guard access by locks.  For context, there are already some extensions
that are not thread-safe even with the GIL, and users already have to
take these sorts of steps.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">PYTHONGIL=1</span></code> override is sometimes useful for debugging.</p>
</section>
</section>
<section id="rationale">
<h2><a class="toc-backref" href="#rationale" role="doc-backlink">Rationale</a></h2>
<section id="non-generational-garbage-collection">
<h3><a class="toc-backref" href="#non-generational-garbage-collection" role="doc-backlink">Non-Generational Garbage Collection</a></h3>
<p>This PEP proposes switching from a generational cyclic garbage
collector to a non-generational collector (when CPython is built
without the GIL). That is equivalent to only having one generation
(the “old” generation). There are two reasons for this proposed
change.</p>
<p>Cyclic garbage collection, even for just the young generation,
requires pausing other threads in the program. The author is
concerned that frequent collections of the young generation would
inhibit efficient scaling in multi-threaded programs. This is a
concern for young generations (but not the old generation) because
the young generations are collected after a fixed number of
allocations, while the collections for the older generation are
scheduled in proportion to the number of live objects in the heap.
Additionally, it is difficult to efficiently keep track of objects in
each generation without the GIL. For example, CPython currently uses
a linked list of objects in each generation. If CPython were to keep
that design, those lists would need to be made thread-safe, and it’s
not clear how to do that efficiently.</p>
<p>Generational garbage collection is used to good effect in many other
language runtimes.  For example, many of the Java HotSpot garbage
collector implementations use multiple generations <a class="footnote-reference brackets" href="#hotspotgc" id="id9">[11]</a>. In
these runtimes, a young generation is frequently a throughput win:
since a large percentage of the young generation is typically “dead,”
the GC is able to reclaim a large amount memory relative to the
amount of work performed. For example, several Java benchmarks show
over 90% of “young” objects are typically collected <a class="footnote-reference brackets" href="#decapo" id="id10">[12]</a>
<a class="footnote-reference brackets" href="#exploitingmemoryjava" id="id11">[13]</a>. This is commonly referred to as the “weak
generational hypothesis;” the observation is that most objects die
young. This pattern is reversed in CPython due to the use of
reference counting.  Although most objects still die young, they are
collected when their reference counts reach zero. Objects that
survive to a garbage collection cycle are most likely to remain
alive <a class="footnote-reference brackets" href="#cpythongc" id="id12">[14]</a>. This difference means that generational
collection is much less effective in CPython than in many other
language runtimes <a class="footnote-reference brackets" href="#golangc" id="id13">[15]</a>.</p>
</section>
<section id="optimistic-avoiding-locking-in-dict-and-list-accesses">
<h3><a class="toc-backref" href="#optimistic-avoiding-locking-in-dict-and-list-accesses" role="doc-backlink">Optimistic Avoiding Locking in <code class="docutils literal notranslate"><span class="pre">dict</span></code> and <code class="docutils literal notranslate"><span class="pre">list</span></code> Accesses</a></h3>
<p>This proposal relies on a scheme that mostly avoids acquiring locks
when accessing individual elements in lists and dictionaries.  Note
that this is not “lock free” in the sense of “lock-free”
and “wait-free” algorithms that guarantee forward progress.  It
simply avoids acquiring locks (mutexes) in the common case to improve
parallelism and reduce overhead.</p>
<p>A much simpler alternative would be to use reader-writer locks to
protect dictionary and list accesses. Reader-writer locks allow
concurrent reads, but not updates, which might seem ideal for list
and dictionaries. The problem is that reader-writer locks have
substantial overhead and poor scalability, particularly when the
critical sections are small, as they are for single-element
dictionary and list accesses <a class="footnote-reference brackets" href="#perfbook" id="id14">[9]</a>. The poor reader
scalability stems from the fact that readers must all update the same
data structure, such as the number of readers in
<code class="docutils literal notranslate"><span class="pre">pthread_rwlocks</span></code>.</p>
<p>The technique described in this PEP is related to RCU
(“read-copy-update”) <a class="footnote-reference brackets" href="#rcu" id="id15">[6]</a> and, to a lesser extent, hazard
pointers, two well-known schemes for optimizing concurrent,
read-mostly data structures. RCU is widely used in the Linux kernel
to protect shared data structures in a scalable manner. Both the
technique in this PEP and RCU work by deferring reclamation while
readers may be accessing the concurrent data structure. RCU is most
commonly used to protect individual objects (like hash tables or
linked lists), while this PEP proposes a scheme to protect larger
blocks of memory (mimalloc “pages”) <a class="footnote-reference brackets" href="#typesafe-rcu" id="id16">[10]</a>.</p>
<p>The need for this scheme is largely due to the use of reference
counting in CPython.  If CPython only relied on a tracing garbage
collector, then this scheme would probably not be necessary because
tracing garbage collectors already defer reclamation in the required
manner.  This would not “solve” scaling issues, but would shift many
of the challenges to the garbage collector implementation.</p>
</section>
</section>
<section id="backwards-compatibility">
<h2><a class="toc-backref" href="#backwards-compatibility" role="doc-backlink">Backwards Compatibility</a></h2>
<p>This PEP poses a number of backwards compatibility issues when
building CPython with the <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> flag, but those issues do
not occur when using the default build configuration.  Nearly all the
backwards compatibility concerns involve the C-API:</p>
<ul class="simple">
<li>CPython builds without the GIL will not be ABI compatible with the
standard CPython build or with the stable ABI due to changes to the
Python object header needed to support biased reference counting.
C-API extensions will need to be rebuilt specifically for this
version.</li>
<li>C-API extensions that rely on the GIL to protect global state or
object state in C code will need additional explicit locking to
remain thread-safe when run without the GIL.</li>
<li>C-API extensions that use borrowed references in ways that are not
safe without the GIL will need to use the equivalent new APIs that
return non-borrowed references. Note that only some uses of
borrowed references are a concern; only references to objects that
might be freed by other threads pose an issue.</li>
<li>Custom memory allocators (<code class="docutils literal notranslate"><span class="pre">PyMem_SetAllocator</span></code>) are required to
delegate the actual allocation to the previously set allocator. For
example, the Python debug allocator and tracing allocators will
continue to work because they delegate the allocation to the
underlying allocator. On the other hand, wholesale replacing of the
allocator (e.g., with jemalloc or tcmalloc) will not work
correctly.</li>
<li>Python objects must be allocated through the standard APIs, such as
<code class="docutils literal notranslate"><span class="pre">PyType_GenericNew</span></code> or <code class="docutils literal notranslate"><span class="pre">PyObject_Malloc</span></code>. Non-Python objects
must <strong>not</strong> be allocated through those APIs. For example, it is
currently acceptable to allocate buffers(non-Python objects)
through <code class="docutils literal notranslate"><span class="pre">PyObject_Malloc</span></code>; that will no longer be allowed and
buffers should instead be allocated through <code class="docutils literal notranslate"><span class="pre">PyMem_Malloc</span></code>,
<code class="docutils literal notranslate"><span class="pre">PyMem_RawMalloc</span></code>, or <code class="docutils literal notranslate"><span class="pre">malloc</span></code>.</li>
</ul>
<p>There are fewer potential backwards compatibility issues for Python
code:</p>
<ul class="simple">
<li>Destructors and weak reference callbacks for code objects and
top-level function objects are delayed until the next cyclic
garbage collection due to the use of deferred reference counting.</li>
<li>Destructors for some objects accessed by multiple threads may be
delayed slightly due to biased reference counting. This is rare:
most objects, even those accessed by multiple threads, are
destroyed immediately as soon as their reference counts are zero.
Two places in the Python standard library tests required
<code class="docutils literal notranslate"><span class="pre">gc.collect()</span></code> calls to continue to pass.</li>
</ul>
</section>
<section id="distribution">
<h2><a class="toc-backref" href="#distribution" role="doc-backlink">Distribution</a></h2>
<p>This PEP poses new challenges for distributing Python.  At least for
some time, there will be two versions of Python requiring separately
compiled C-API extensions.  It may take some time for C-API extension
authors to build <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> compatible packages and upload
them to PyPI.  Additionally, some authors may be hesitant to support
the <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code>  mode until it has wide adoption, but adoption
will likely depend on the availability of Python’s rich set of
extensions.</p>
<p>To mitigate this, the author will work with Anaconda to distribute
a <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> version of Python together with compatible
packages from conda channels.  This centralizes the challenges of
building extensions, and the author believes this will enable more
people to use Python without the GIL sooner than they would otherwise
be able to.</p>
</section>
<section id="performance">
<h2><a class="toc-backref" href="#performance" role="doc-backlink">Performance</a></h2>
<p>The changes to make CPython thread-safe without the GIL increase
execution overhead for <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> builds.  The performance
impact is different for programs that use only a single thread compared
to programs that use multiple threads, so the table below reports
execution overhead separately for these types of programs separately.</p>
<table class="docutils align-default" id="id29">
<caption><span class="caption-text">Execution Overhead on pyperformance 1.0.6</span></caption>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head">Intel Skylake</th>
<th class="head">AMD Zen 3</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td>One thread</td>
<td>6%</td>
<td>5%</td>
</tr>
<tr class="row-odd"><td>Multiple threads</td>
<td>8%</td>
<td>7%</td>
</tr>
</tbody>
</table>
<p>The baseline used to measure overhead is <code class="docutils literal notranslate"><span class="pre">018be4c</span></code> from <a class="reference external" href="https://github.com/python/cpython/pull/19474">PR 19474</a>,
which implements immortal objects for Python 3.12.  The largest
contribution to execution overhead is biased reference counting
followed by per-object locking.  For thread-safety reasons, an
application running with multiple threads will only specialize a given
bytecode once; this is why the overhead for programs that use multiple
threads is larger compared to programs that only use one thread.
However, with the GIL disabled, programs that use multiple threads
should also be able to more effectively use multiple CPU cores.</p>
<p>Note that this PEP would not affect the performance of the default
(non <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code>) builds of CPython.</p>
</section>
<section id="build-bots">
<h2><a class="toc-backref" href="#build-bots" role="doc-backlink">Build Bots</a></h2>
<p>The stable build bots will also include <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> builds.</p>
</section>
<section id="how-to-teach-this">
<h2><a class="toc-backref" href="#how-to-teach-this" role="doc-backlink">How to Teach This</a></h2>
<p>As part of implementing the <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> mode, the author will
write a “HOWTO” guide <a class="footnote-reference brackets" href="#howto" id="id17">[18]</a> for making packages compatible when
running Python without the GIL.</p>
</section>
<section id="reference-implementation">
<h2><a class="toc-backref" href="#reference-implementation" role="doc-backlink">Reference Implementation</a></h2>
<p>There are two GitHub repositories implementing versions of CPython
without the GIL:</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/colesbury/nogil-3.12">https://github.com/colesbury/nogil-3.12</a></li>
<li><a class="reference external" href="https://github.com/colesbury/nogil">https://github.com/colesbury/nogil</a></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">nogil-3.12</span></code> is based on Python 3.12.0a4.  It is useful for
evaluating single-threaded execution overhead and as a reference
implementation for this PEP.  It is less useful for evaluating C-API
extension compatibility because many extensions are not currently
compatible with Python 3.12.  Due to limited time for the 3.12 port,
the <code class="docutils literal notranslate"><span class="pre">nogil-3.12</span></code> implementation does not skip all deferred reference
counts.  As a temporary work around, the implementation immortalizes
objects that use deferred reference counting in programs that spawn
multiple threads.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">nogil</span></code> repository is based on Python 3.9.10.  It is useful for
evaluating multi-threading scaling in real world applications and
extension compatibility.  It is more stable and well tested than the
<code class="docutils literal notranslate"><span class="pre">nogil-3.12</span></code> repository.</p>
</section>
<section id="alternatives">
<h2><a class="toc-backref" href="#alternatives" role="doc-backlink">Alternatives</a></h2>
<p>Python currently supports a number of ways to enable parallelism, but
the existing techniques come with significant limitations.</p>
<section id="multiprocessing">
<h3><a class="toc-backref" href="#multiprocessing" role="doc-backlink">Multiprocessing</a></h3>
<p>The multiprocessing library allows Python programs to start and
communicate with Python subprocesses.  This allows for parallelism
because each subprocess has its own Python interpreter (i.e., there’s
one GIL per process).  Multiprocessing has a few substantial
limitations.  Communication between processes is limited: objects
generally need to be serialized or copied to shared memory.  This
introduces overhead (due to serialization) and complicates building
APIs on top of multiprocessing.  Starting a subprocess is also more
expensive than starting a thread, especially with the “spawn”
implementation.  Starting a thread takes ~100 µs, while spawning a
subprocess takes ~50 ms (50,000 µs) due to Python re-initialization.</p>
<p>Finally, many C and C++ libraries support access from multiple
threads but do not support access or use across multiple processes.</p>
</section>
<section id="releasing-the-gil-in-c-api-extensions">
<h3><a class="toc-backref" href="#releasing-the-gil-in-c-api-extensions" role="doc-backlink">Releasing the GIL in C-API Extensions</a></h3>
<p>C-API extensions can release the GIL around long running functions.
This allows for some degree of parallelism, since multiple threads
can run concurrently when the GIL is released, but the overhead of
acquiring and releasing the GIL typically prevents this from scaling
efficiently beyond a few threads.  Many scientific computing
libraries release the GIL in computational heavy functions, and the
CPython standard library releases the GIL around blocking I/O.</p>
</section>
<section id="internal-parallelization">
<h3><a class="toc-backref" href="#internal-parallelization" role="doc-backlink">Internal Parallelization</a></h3>
<p>Functions implemented in C may use multiple threads internally. For
example, Intel’s NumPy distribution, PyTorch, and TensorFlow all use
this technique to internally parallelize individual operations. This
works well when the basic operations are large enough to be
parallelized efficiently, but not when there are many small
operations or when the operations depend on some Python code. Calling
into Python from C requires acquiring the GIL – even short snippets
of Python code can inhibit scaling.</p>
</section>
</section>
<section id="related-work">
<h2><a class="toc-backref" href="#related-work" role="doc-backlink">Related Work</a></h2>
<section id="per-interpreter-gil">
<h3><a class="toc-backref" href="#per-interpreter-gil" role="doc-backlink">Per-Interpreter GIL</a></h3>
<p>The recently accepted <a class="pep reference internal" href="../pep-0684/" title="PEP 684 – A Per-Interpreter GIL">PEP 684</a> proposes a per-interpreter GIL to
address multi-core parallelism.  This would allow parallelism between
interpreters in the same process, but places substantial restrictions
on sharing Python data between interpreters.  Both this PEP
and <a class="pep reference internal" href="../pep-0684/" title="PEP 684 – A Per-Interpreter GIL">PEP 684</a> address the multi-core parallelism, but with different
tradeoffs and techniques.  It is feasible to implement both PEPs in
CPython at the same time.</p>
</section>
<section id="gilectomy">
<h3><a class="toc-backref" href="#gilectomy" role="doc-backlink">Gilectomy</a></h3>
<p>Gilectomy <a class="footnote-reference brackets" href="#id27" id="id18">[20]</a> was a project by Larry Hastings to remove the
GIL in CPython.  Like the design proposed by this PEP, the Gilectomy
supported multiple threads running in parallel within the same
interpreter (i.e., “free-threading”) and made use of fine-grained
locking.  The reference implementation in this PEP improves on
single-threaded performance and scalability compared to the
Gilectomy.</p>
</section>
<section id="pyparallel">
<h3><a class="toc-backref" href="#pyparallel" role="doc-backlink">PyParallel</a></h3>
<p>PyParallel <a class="footnote-reference brackets" href="#id28" id="id19">[21]</a> was a proof-of-concept fork of Python 3.3 by
Trent Nelson that supported multiple threads running simultaneously
in a single Python process.  The fork introduced the concept
of “parallel threads” – threads that can run simultaneously while
the main Python thread is suspended.  Parallel threads had read-only
access to objects created by the main thread.  Objects created within
parallel threads lived for the lifetime of the creating thread.  For
HTTP servers, this might correspond to the lifetime of a request.</p>
</section>
<section id="python-safethread">
<h3><a class="toc-backref" href="#python-safethread" role="doc-backlink">python-safethread</a></h3>
<p>The python-safethread <a class="footnote-reference brackets" href="#pythonsafethread" id="id20">[22]</a> project was a patch to
Python 3.0 by Adam Olsen to remove the GIL.  Some aspects of the
project are similar to the design proposed by this PEP.  Both use
fine-grained locking and optimize reference counting for cases
where the object is created and accessed by the same thread.</p>
</section>
<section id="greg-stein-s-free-threading-patch">
<h3><a class="toc-backref" href="#greg-stein-s-free-threading-patch" role="doc-backlink">Greg Stein’s Free-Threading Patch</a></h3>
<p>In 1996, Greg Stein published a patch against Python 1.4 that removed
the GIL <a class="footnote-reference brackets" href="#gsteinpatch" id="id21">[23]</a>.  The patch used atomic reference counting on
Windows and a global reference count lock on Linux. List and
dictionary accesses were protected by mutexes.  Parts of the patch
were adopted in CPython. In particular, the patch introduced a
PyThreadState structure and correct per-thread exception handling.</p>
<p>Dave Beazley revisited the patch in a 2011 blog post <a class="footnote-reference brackets" href="#dabeaz" id="id22">[24]</a>.</p>
</section>
<section id="jython-and-ironpython">
<h3><a class="toc-backref" href="#jython-and-ironpython" role="doc-backlink">Jython and IronPython</a></h3>
<p>Some alternative Python implementations like Jython <a class="footnote-reference brackets" href="#jython" id="id23">[25]</a> and
IronPython <a class="footnote-reference brackets" href="#ironpython" id="id24">[26]</a> do not have a global interpreter lock.
However, they do not support CPython extensions. (The implementations
can interface with code written in Java or C#).</p>
</section>
<section id="pypy-stm">
<h3><a class="toc-backref" href="#pypy-stm" role="doc-backlink">PyPy-STM</a></h3>
<p>The pypy-stm <a class="footnote-reference brackets" href="#pypystm" id="id25">[27]</a> interpreter is a variant of PyPy that uses
software transactional memory.  The authors report single-threaded
performance overhead in the 20%-50% range compared to PyPy.  It is
not compatible with CPython extensions.</p>
</section>
</section>
<section id="rejected-ideas">
<h2><a class="toc-backref" href="#rejected-ideas" role="doc-backlink">Rejected Ideas</a></h2>
<section id="why-not-use-a-concurrent-garbage-collector">
<h3><a class="toc-backref" href="#why-not-use-a-concurrent-garbage-collector" role="doc-backlink">Why Not Use a Concurrent Garbage Collector?</a></h3>
<p>Many recent garbage collectors are mostly concurrent – they avoid long
stop-the-world pauses by allowing the garbage collector to run
concurrently with the application. So why not use a concurrent
collector?</p>
<p>Concurrent collection requires write barriers (or read barriers).  The
author is not aware of a way to add write barriers to CPython without
substantially breaking the C-API.</p>
</section>
<section id="why-not-deprecate-pydict-getitem-in-favor-of-pydict-fetchitem">
<h3><a class="toc-backref" href="#why-not-deprecate-pydict-getitem-in-favor-of-pydict-fetchitem" role="doc-backlink">Why Not Deprecate <code class="docutils literal notranslate"><span class="pre">PyDict_GetItem</span></code> in Favor of <code class="docutils literal notranslate"><span class="pre">PyDict_FetchItem</span></code>?</a></h3>
<p>This PEP proposes a new API <code class="docutils literal notranslate"><span class="pre">PyDict_FetchItem</span></code> which behaves like
<code class="docutils literal notranslate"><span class="pre">PyDict_GetItem</span></code>, but returns a new reference instead of a borrowed
reference.  As described in <a class="reference internal" href="#borrowed-references">Borrowed References</a>, some uses of
borrowed references that were safe when running with the GIL are
unsafe when running without the GIL and need to be replaced by
functions like <code class="docutils literal notranslate"><span class="pre">PyDict_FetchItem</span></code> that return new references.</p>
<p>This PEP does <em>not</em> propose deprecating <code class="docutils literal notranslate"><span class="pre">PyDict_GetItem</span></code> and similar
functions that return borrowed references for a few reasons:</p>
<ul class="simple">
<li>Many of the uses of borrowed references are safe, even when running
without the GIL.  For example, C API functions often use
<code class="docutils literal notranslate"><span class="pre">PyDict_GetItem</span></code> to retrieve items from the keyword
argument dictionary.  These calls are safe because the keyword
argument dictionary is only visible to a single thread.</li>
<li>I tried this approach early on and found that wholesale replacing of
<code class="docutils literal notranslate"><span class="pre">PyDict_GetItem</span></code> with <code class="docutils literal notranslate"><span class="pre">PyDict_FetchItem</span></code> frequently introduced
new reference counting bugs.  In my opinion, the risk of
introducing new reference counting bugs generally outweighs the
risks of missing a <code class="docutils literal notranslate"><span class="pre">PyDict_GetItem</span></code> call that is unsafe without
the GIL.</li>
</ul>
</section>
<section id="why-not-use-pep-683-immortalization">
<h3><a class="toc-backref" href="#why-not-use-pep-683-immortalization" role="doc-backlink">Why Not Use PEP 683 Immortalization?</a></h3>
<p>Like <a class="pep reference internal" href="../pep-0683/" title="PEP 683 – Immortal Objects, Using a Fixed Refcount">PEP 683</a>, this PEP proposes an immortalization scheme for
Python objects, but the PEPs use different bit representations to
mark immortal objects.  The schemes cannot be identical because this
PEP depends on biased reference counting, which has two reference
count fields instead of one.</p>
</section>
</section>
<section id="open-issues">
<h2><a class="toc-backref" href="#open-issues" role="doc-backlink">Open Issues</a></h2>
<section id="improved-specialization">
<h3><a class="toc-backref" href="#improved-specialization" role="doc-backlink">Improved Specialization</a></h3>
<p>The Python 3.11 release introduced quickening and specialization as part
of the faster CPython project, substantially improving performance.
Specialization replaces slow bytecode instructions with faster
variants <a class="footnote-reference brackets" href="#pep659" id="id26">[19]</a>.  To maintain thread-safety, applications that use
multiple threads (and run without the GIL) will only specialize each
bytecode once, which can lower performance on some programs.  It is
possible to support specializing multiple times, but that requires more
investigation and is not part of this PEP.</p>
</section>
<section id="python-build-modes">
<h3><a class="toc-backref" href="#python-build-modes" role="doc-backlink">Python Build Modes</a></h3>
<p>This PEP introduces a new build mode (<code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code>) that is not
ABI compatible with the standard build mode.  The additional build
mode adds complexity for both Python core developers and extension
developers.  The author believes a worthwhile goal is to combine
these build modes and have the global interpreter lock controlled at
runtime, possibly disabled by default.  The path to this goal remains
an open issue, but a possible path might look like the following:</p>
<ol class="arabic simple">
<li>In 2024, CPython 3.13 is released with support for a
<code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> build time flag.  There are two ABIs for
CPython, one with the GIL and one without.  Extension authors
target both ABIs.</li>
<li>After 2–3 releases, (i.e., in 2026–2027), CPython is released
with the GIL controlled by a runtime environment variable or
flag. The GIL is enabled by default.  There is only a single ABI.</li>
<li>After another 2–3 release (i.e., 2028–2030), CPython switches to
the GIL being disabled by default.  The GIL can still be enabled
at runtime via an environment variable or command line flag.</li>
</ol>
<p>This PEP covers the first step, with the remaining steps left as open
issues.  In this scenario, there would be a two to three year period
where extension authors would target an extra CPython build per
supported CPU architecture and OS.</p>
</section>
<section id="integration">
<h3><a class="toc-backref" href="#integration" role="doc-backlink">Integration</a></h3>
<p>The reference implementation changes approximately 15,000 lines of code
in CPython and includes mimalloc, which is also approximately 15,000
lines of code.  Most changes are not performance sensitive and can be
included in both <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> and the default builds.  Some
macros, like <code class="docutils literal notranslate"><span class="pre">Py_BEGIN_CRITICAL_SECTION</span></code> will be no-ops in the
default build.  Thee author does not expect a huge number of <code class="docutils literal notranslate"><span class="pre">#ifdef</span></code>
statements to support the <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> builds.</p>
</section>
<section id="mitigations-for-single-threaded-performance">
<h3><a class="toc-backref" href="#mitigations-for-single-threaded-performance" role="doc-backlink">Mitigations for Single-Threaded Performance</a></h3>
<p>The changes proposed in the PEP will increase execution overhead for
<code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> builds compared to Python builds with the GIL.  In
other words, it will have slower single-threaded performance.  There
are some possible optimizations to reduce execution overhead,
especially for <code class="docutils literal notranslate"><span class="pre">--disable-gil</span></code> builds that only use a single
thread.  These may be worthwhile if a longer term goal is to have a
single build mode, but the choice of optimizations and their
trade-offs remain an open issue.</p>
</section>
</section>
<section id="references">
<h2><a class="toc-backref" href="#references" role="doc-backlink">References</a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="yuemmwang2019" role="doc-footnote">
<dt class="label" id="yuemmwang2019">[<a href="#id1">1</a>]</dt>
<dd>“Exploiting Parallelism Opportunities with Deep Learning Frameworks.”
Yu Emma Wang, Carole-Jean Wu, Xiaodong Wang, Kim Hazelwood, David Brooks. 2019.
<a class="reference external" href="https://arxiv.org/abs/1908.04705">https://arxiv.org/abs/1908.04705</a>.</aside>
<aside class="footnote brackets" id="torchdeploy" role="doc-footnote">
<dt class="label" id="torchdeploy">[<a href="#id2">2</a>]</dt>
<dd>“Using Python for Model Inference in Deep Learning.”
Zachary DeVito, Jason Ansel, Will Constable, Michael Suo, Ailing Zhang, Kim Hazelwood. 2021.
<a class="reference external" href="https://arxiv.org/abs/2104.00254">https://arxiv.org/abs/2104.00254</a>. See Figure 5.</aside>
<aside class="footnote brackets" id="brc" role="doc-footnote">
<dt class="label" id="brc">[<a href="#id3">3</a>]</dt>
<dd>“Biased reference counting: minimizing atomic operations in garbage collection”.
Jiho Choi, Thomas Shull, and Josep Torrellas. PACT 2018.
<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3243176.3243195">https://dl.acm.org/doi/abs/10.1145/3243176.3243195</a>.</aside>
<aside class="footnote brackets" id="pep683" role="doc-footnote">
<dt class="label" id="pep683">[4]</dt>
<dd><a class="pep reference internal" href="../pep-0683/" title="PEP 683 – Immortal Objects, Using a Fixed Refcount">PEP 683</a> – Immortal Objects, Using a Fixed Refcount.</aside>
<aside class="footnote brackets" id="tid" role="doc-footnote">
<dt class="label" id="tid">[<a href="#id4">5</a>]</dt>
<dd><a class="reference external" href="https://github.com/colesbury/nogil/blob/f7e45d6bfbbd48c8d5cf851c116b73b85add9fc6/Include/object.h#L428-L455">https://github.com/colesbury/nogil/blob/f7e45d6bfbbd48c8d5cf851c116b73b85add9fc6/Include/object.h#L428-L455</a>.</aside>
<aside class="footnote brackets" id="rcu" role="doc-footnote">
<dt class="label" id="rcu">[6]<em> (<a href='#id6'>1</a>, <a href='#id15'>2</a>) </em></dt>
<dd>“What is RCU, Fundamentally?”
Paul E. McKenney, Jonathan Walpole. 2017.
<a class="reference external" href="https://lwn.net/Articles/262464/">https://lwn.net/Articles/262464/</a></aside>
<aside class="footnote brackets" id="heaps" role="doc-footnote">
<dt class="label" id="heaps">[<a href="#id7">7</a>]</dt>
<dd>There are two heaps for Python objects because PyObjects
that support cyclic garbage collection have extra fields preceding
the PyObject struct.</aside>
<aside class="footnote brackets" id="gus" role="doc-footnote">
<dt class="label" id="gus">[<a href="#id8">8</a>]</dt>
<dd>“Global Unbounded Sequences (GUS)”
<a class="reference external" href="https://github.com/freebsd/freebsd-src/blob/9408f36627b74a472dc82f7a43320235c0c9055a/sys/kern/subr_smr.c#L44">https://github.com/freebsd/freebsd-src/blob/9408f36627b74a472dc82f7a43320235c0c9055a/sys/kern/subr_smr.c#L44</a>.
See also <a class="reference external" href="https://people.kernel.org/joelfernandes/gus-vs-rcu">https://people.kernel.org/joelfernandes/gus-vs-rcu</a>.</aside>
<aside class="footnote brackets" id="perfbook" role="doc-footnote">
<dt class="label" id="perfbook">[<a href="#id14">9</a>]</dt>
<dd>“Is Parallel Programming Hard, And, If So, What Can You Do About It?”
Paul E. McKenney. 2022.
<a class="reference external" href="https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html">https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html</a>.</aside>
<aside class="footnote brackets" id="typesafe-rcu" role="doc-footnote">
<dt class="label" id="typesafe-rcu">[<a href="#id16">10</a>]</dt>
<dd><code class="docutils literal notranslate"><span class="pre">SLAB_TYPESAFE_BY_RCU</span></code> is an example in which RCU
protects blocks of memory and not any individual object.  See
<a class="reference external" href="https://www.kernel.org/doc/html/latest/RCU/whatisRCU.html#analogy-with-reference-counting">https://www.kernel.org/doc/html/latest/RCU/whatisRCU.html#analogy-with-reference-counting</a>.</aside>
<aside class="footnote brackets" id="hotspotgc" role="doc-footnote">
<dt class="label" id="hotspotgc">[<a href="#id9">11</a>]</dt>
<dd>“HotSpot Virtual Machine Garbage Collection Tuning Guide.”
<a class="reference external" href="https://docs.oracle.com/en/java/javase/12/gctuning/hotspot-virtual-machine-garbage-collection-tuning-guide.pdf">https://docs.oracle.com/en/java/javase/12/gctuning/hotspot-virtual-machine-garbage-collection-tuning-guide.pdf</a>.
Most of the hotspot garbage collectors are generational, with the
notable exception of ZGC, although there is ongoing work to make
that generational.</aside>
<aside class="footnote brackets" id="decapo" role="doc-footnote">
<dt class="label" id="decapo">[<a href="#id10">12</a>]</dt>
<dd><a class="reference external" href="https://openresearch-repository.anu.edu.au/bitstream/1885/33723/2/01_Blackburn_The_DaCapo_Benchmarks:_Java_2006.pdf">The DaCapo Benchmarks: Java Benchmarking Development and
Analysis</a>.
See column “Nursery Survival” in Table 4.</aside>
<aside class="footnote brackets" id="exploitingmemoryjava" role="doc-footnote">
<dt class="label" id="exploitingmemoryjava">[<a href="#id11">13</a>]</dt>
<dd>“Exploiting memory usage patterns to improve garbage collections in Java.”
<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/1852761.1852768">https://dl.acm.org/doi/abs/10.1145/1852761.1852768</a>.</aside>
<aside class="footnote brackets" id="cpythongc" role="doc-footnote">
<dt class="label" id="cpythongc">[<a href="#id12">14</a>]</dt>
<dd>“most things usually turn out to be reachable”
<a class="reference external" href="https://github.com/python/cpython/blob/cd6655a8589e99ae4088b3bed4a692a19ed48779/Modules/gcmodule.c#L1106">https://github.com/python/cpython/blob/cd6655a8589e99ae4088b3bed4a692a19ed48779/Modules/gcmodule.c#L1106</a>.</aside>
<aside class="footnote brackets" id="golangc" role="doc-footnote">
<dt class="label" id="golangc">[<a href="#id13">15</a>]</dt>
<dd>The Go team observed something similar in Go, but due to
escape analysis and pass-by-value instead of reference
counting. Recent versions of Go use a non-generational garbage
collector. <a class="reference external" href="https://go.dev/blog/ismmkeynote">https://go.dev/blog/ismmkeynote</a>.</aside>
<aside class="footnote brackets" id="nogil" role="doc-footnote">
<dt class="label" id="nogil">[16]</dt>
<dd><a class="reference external" href="https://github.com/colesbury/nogil">https://github.com/colesbury/nogil</a>.</aside>
<aside class="footnote brackets" id="nogil312" role="doc-footnote">
<dt class="label" id="nogil312">[<a href="#id5">17</a>]</dt>
<dd><a class="reference external" href="https://github.com/colesbury/nogil-3.12">https://github.com/colesbury/nogil-3.12</a>.</aside>
<aside class="footnote brackets" id="howto" role="doc-footnote">
<dt class="label" id="howto">[<a href="#id17">18</a>]</dt>
<dd>Python HOWTOs.
<a class="reference external" href="https://docs.python.org/3/howto/index.html">https://docs.python.org/3/howto/index.html</a>.</aside>
<aside class="footnote brackets" id="pep659" role="doc-footnote">
<dt class="label" id="pep659">[<a href="#id26">19</a>]</dt>
<dd><a class="pep reference internal" href="../pep-0659/" title="PEP 659 – Specializing Adaptive Interpreter">PEP 659</a> – Specializing Adaptive Interpreter.</aside>
<aside class="footnote brackets" id="id27" role="doc-footnote">
<dt class="label" id="id27">[<a href="#id18">20</a>]</dt>
<dd>Gilectomy.
Larry Hastings. 2016.
<a class="reference external" href="https://github.com/larryhastings/gilectomy/tree/gilectomy">https://github.com/larryhastings/gilectomy/tree/gilectomy</a>.</aside>
<aside class="footnote brackets" id="id28" role="doc-footnote">
<dt class="label" id="id28">[<a href="#id19">21</a>]</dt>
<dd>PyParallel.
Trent Nelson. 2016.
<a class="reference external" href="http://pyparallel.org/">http://pyparallel.org/</a>.</aside>
<aside class="footnote brackets" id="pythonsafethread" role="doc-footnote">
<dt class="label" id="pythonsafethread">[<a href="#id20">22</a>]</dt>
<dd>python-safethread.
Adam Olsen. 2008.
<a class="reference external" href="https://launchpad.net/python-safethread">https://launchpad.net/python-safethread</a></aside>
<aside class="footnote brackets" id="gsteinpatch" role="doc-footnote">
<dt class="label" id="gsteinpatch">[<a href="#id21">23</a>]</dt>
<dd><a class="reference external" href="https://www.python.org/ftp/python/contrib-09-Dec-1999/System/threading.tar.gz">https://www.python.org/ftp/python/contrib-09-Dec-1999/System/threading.tar.gz</a>.</aside>
<aside class="footnote brackets" id="dabeaz" role="doc-footnote">
<dt class="label" id="dabeaz">[<a href="#id22">24</a>]</dt>
<dd>An Inside Look at the GIL Removal Patch of Lore.
David Beazley. 2011.
<a class="reference external" href="https://dabeaz.blogspot.com/2011/08/inside-look-at-gil-removal-patch-of.html">https://dabeaz.blogspot.com/2011/08/inside-look-at-gil-removal-patch-of.html</a>.</aside>
<aside class="footnote brackets" id="jython" role="doc-footnote">
<dt class="label" id="jython">[<a href="#id23">25</a>]</dt>
<dd>Jython.
<a class="reference external" href="https://www.jython.org/">https://www.jython.org/</a></aside>
<aside class="footnote brackets" id="ironpython" role="doc-footnote">
<dt class="label" id="ironpython">[<a href="#id24">26</a>]</dt>
<dd>IronPython.
<a class="reference external" href="https://ironpython.net/">https://ironpython.net/</a></aside>
<aside class="footnote brackets" id="pypystm" role="doc-footnote">
<dt class="label" id="pypystm">[<a href="#id25">27</a>]</dt>
<dd>PyPy: Software Transactional Memory.
<a class="reference external" href="https://doc.pypy.org/en/latest/stm.html">https://doc.pypy.org/en/latest/stm.html</a></aside>
</aside>
</section>
<section id="acknowledgments">
<h2><a class="toc-backref" href="#acknowledgments" role="doc-backlink">Acknowledgments</a></h2>
<p>Thanks to Hugh Leather, Łukasz Langa, and Eric Snow for providing
feedback on drafts of this PEP.</p>
</section>
<section id="copyright">
<h2><a class="toc-backref" href="#copyright" role="doc-backlink">Copyright</a></h2>
<p>This document is placed in the public domain or under the
CC0-1.0-Universal license, whichever is more permissive.</p>
</section>
</section>
<hr class="docutils" />
<p>Source: <a class="reference external" href="https://github.com/python/peps/blob/main/peps/pep-0703.rst">https://github.com/python/peps/blob/main/peps/pep-0703.rst</a></p>
<p>Last modified: <a class="reference external" href="https://github.com/python/peps/commits/main/peps/pep-0703.rst">2025-02-01 08:55:40 GMT</a></p>

        </article>
        <nav id="pep-sidebar">
            <h2>Contents</h2>
            <ul>
<li><a class="reference internal" href="#abstract">Abstract</a></li>
<li><a class="reference internal" href="#motivation">Motivation</a><ul>
<li><a class="reference internal" href="#the-gil-makes-many-types-of-parallelism-difficult-to-express">The GIL Makes Many Types of Parallelism Difficult to Express</a></li>
<li><a class="reference internal" href="#the-gil-affects-python-library-usability">The GIL Affects Python Library Usability</a></li>
<li><a class="reference internal" href="#gpu-heavy-workloads-require-multi-core-processing">GPU-Heavy Workloads Require Multi-Core Processing</a></li>
<li><a class="reference internal" href="#the-gil-makes-deploying-python-ai-models-difficult">The GIL Makes Deploying Python AI Models Difficult</a></li>
<li><a class="reference internal" href="#motivation-summary">Motivation Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#specification">Specification</a><ul>
<li><a class="reference internal" href="#build-configuration-changes">Build Configuration Changes</a></li>
<li><a class="reference internal" href="#overview-of-cpython-changes">Overview of CPython Changes</a></li>
<li><a class="reference internal" href="#reference-counting">Reference Counting</a><ul>
<li><a class="reference internal" href="#immortalization">Immortalization</a></li>
<li><a class="reference internal" href="#biased-reference-counting">Biased Reference Counting</a><ul>
<li><a class="reference internal" href="#default-0b00">Default (<code class="docutils literal notranslate"><span class="pre">0b00</span></code>)</a></li>
<li><a class="reference internal" href="#weakrefs-0b01">Weakrefs (<code class="docutils literal notranslate"><span class="pre">0b01</span></code>)</a></li>
<li><a class="reference internal" href="#queued-0b10">Queued (<code class="docutils literal notranslate"><span class="pre">0b10</span></code>)</a></li>
<li><a class="reference internal" href="#merged-0b11">Merged (<code class="docutils literal notranslate"><span class="pre">0b11</span></code>)</a></li>
<li><a class="reference internal" href="#reference-counting-pseudo-code">Reference counting pseudo-code</a></li>
</ul>
</li>
<li><a class="reference internal" href="#deferred-reference-counting">Deferred Reference Counting</a></li>
<li><a class="reference internal" href="#garbage-collector-modifications-for-deferred-reference-counting">Garbage Collector Modifications for Deferred Reference Counting</a></li>
<li><a class="reference internal" href="#reference-counting-type-objects">Reference Counting Type Objects</a></li>
</ul>
</li>
<li><a class="reference internal" href="#memory-management">Memory Management</a><ul>
<li><a class="reference internal" href="#cpython-free-lists">CPython Free Lists</a></li>
</ul>
</li>
<li><a class="reference internal" href="#garbage-collection-cycle-collection">Garbage Collection (Cycle Collection)</a><ul>
<li><a class="reference internal" href="#stop-the-world">Stop-the-World</a></li>
<li><a class="reference internal" href="#thread-states">Thread States</a></li>
<li><a class="reference internal" href="#generations">Generations</a></li>
<li><a class="reference internal" href="#integration-with-deferred-and-biased-reference-counting">Integration With Deferred and Biased Reference Counting</a></li>
</ul>
</li>
<li><a class="reference internal" href="#container-thread-safety">Container Thread-Safety</a><ul>
<li><a class="reference internal" href="#borrowed-references">Borrowed References</a></li>
<li><a class="reference internal" href="#python-critical-sections">Python Critical Sections</a></li>
<li><a class="reference internal" href="#optimistically-avoiding-locking">Optimistically Avoiding Locking</a></li>
<li><a class="reference internal" href="#mimalloc-changes-for-optimistic-list-and-dict-access">Mimalloc Changes for Optimistic <code class="docutils literal notranslate"><span class="pre">list</span></code> and <code class="docutils literal notranslate"><span class="pre">dict</span></code> Access</a></li>
<li><a class="reference internal" href="#mimalloc-page-reuse">Mimalloc Page Reuse</a></li>
<li><a class="reference internal" href="#optimistic-dict-and-list-access-summary">Optimistic <code class="docutils literal notranslate"><span class="pre">dict</span></code> and <code class="docutils literal notranslate"><span class="pre">list</span></code> Access Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#specializing-interpreter">Specializing Interpreter</a></li>
<li><a class="reference internal" href="#py-mod-gil-slot"><code class="docutils literal notranslate"><span class="pre">Py_mod_gil</span></code> Slot</a></li>
<li><a class="reference internal" href="#pythongil-environment-variable"><code class="docutils literal notranslate"><span class="pre">PYTHONGIL</span></code> Environment Variable</a></li>
</ul>
</li>
<li><a class="reference internal" href="#rationale">Rationale</a><ul>
<li><a class="reference internal" href="#non-generational-garbage-collection">Non-Generational Garbage Collection</a></li>
<li><a class="reference internal" href="#optimistic-avoiding-locking-in-dict-and-list-accesses">Optimistic Avoiding Locking in <code class="docutils literal notranslate"><span class="pre">dict</span></code> and <code class="docutils literal notranslate"><span class="pre">list</span></code> Accesses</a></li>
</ul>
</li>
<li><a class="reference internal" href="#backwards-compatibility">Backwards Compatibility</a></li>
<li><a class="reference internal" href="#distribution">Distribution</a></li>
<li><a class="reference internal" href="#performance">Performance</a></li>
<li><a class="reference internal" href="#build-bots">Build Bots</a></li>
<li><a class="reference internal" href="#how-to-teach-this">How to Teach This</a></li>
<li><a class="reference internal" href="#reference-implementation">Reference Implementation</a></li>
<li><a class="reference internal" href="#alternatives">Alternatives</a><ul>
<li><a class="reference internal" href="#multiprocessing">Multiprocessing</a></li>
<li><a class="reference internal" href="#releasing-the-gil-in-c-api-extensions">Releasing the GIL in C-API Extensions</a></li>
<li><a class="reference internal" href="#internal-parallelization">Internal Parallelization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#related-work">Related Work</a><ul>
<li><a class="reference internal" href="#per-interpreter-gil">Per-Interpreter GIL</a></li>
<li><a class="reference internal" href="#gilectomy">Gilectomy</a></li>
<li><a class="reference internal" href="#pyparallel">PyParallel</a></li>
<li><a class="reference internal" href="#python-safethread">python-safethread</a></li>
<li><a class="reference internal" href="#greg-stein-s-free-threading-patch">Greg Stein’s Free-Threading Patch</a></li>
<li><a class="reference internal" href="#jython-and-ironpython">Jython and IronPython</a></li>
<li><a class="reference internal" href="#pypy-stm">PyPy-STM</a></li>
</ul>
</li>
<li><a class="reference internal" href="#rejected-ideas">Rejected Ideas</a><ul>
<li><a class="reference internal" href="#why-not-use-a-concurrent-garbage-collector">Why Not Use a Concurrent Garbage Collector?</a></li>
<li><a class="reference internal" href="#why-not-deprecate-pydict-getitem-in-favor-of-pydict-fetchitem">Why Not Deprecate <code class="docutils literal notranslate"><span class="pre">PyDict_GetItem</span></code> in Favor of <code class="docutils literal notranslate"><span class="pre">PyDict_FetchItem</span></code>?</a></li>
<li><a class="reference internal" href="#why-not-use-pep-683-immortalization">Why Not Use PEP 683 Immortalization?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#open-issues">Open Issues</a><ul>
<li><a class="reference internal" href="#improved-specialization">Improved Specialization</a></li>
<li><a class="reference internal" href="#python-build-modes">Python Build Modes</a></li>
<li><a class="reference internal" href="#integration">Integration</a></li>
<li><a class="reference internal" href="#mitigations-for-single-threaded-performance">Mitigations for Single-Threaded Performance</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references">References</a></li>
<li><a class="reference internal" href="#acknowledgments">Acknowledgments</a></li>
<li><a class="reference internal" href="#copyright">Copyright</a></li>
</ul>

            <br>
            <a id="source" href="https://github.com/python/peps/blob/main/peps/pep-0703.rst?plain=1">Page Source (GitHub)</a>
        </nav>
    </section>
    <script src="../_static/colour_scheme.js"></script>
    <script src="../_static/wrap_tables.js"></script>
    <script src="../_static/sticky_banner.js"></script>
    <script src="https://analytics.python.org/js/script.outbound-links.js"
            data-domain="peps.python.org" defer></script>
</body>
</html>