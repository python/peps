PEP: 768
Title: Safe external debugger interface for CPython
Author: Pablo Galindo Salgado <pablogsal@python.org>, Matt Wozniski <godlygeek@gmail.com>, Ivona Stojanovic <stojanovic.i@hotmail.com>
Discussions-To: https://discuss.python.org/t/pep-768-safe-external-debugger-interface-for-cpython/73969
Status: Draft
Type: Standards Track
Created: 25-Nov-2024
Python-Version: 3.14
Post-History: `11-Dec-2024 <https://discuss.python.org/t/pep-768-safe-external-debugger-interface-for-cpython/73969>`__

Abstract
========

This PEP proposes adding a zero-overhead debugging interface to CPython that
allows debuggers and profilers to safely attach to running Python processes. The
interface provides safe execution points for attaching debugger code without
modifying the interpreter's normal execution path or adding runtime overhead.

A key application of this interface will be enabling pdb to attach to live
processes by process ID, similar to ``gdb -p``, allowing developers to inspect and
debug Python applications interactively in real-time without stopping or
restarting them.

Motivation
==========


Debugging Python processes in production and live environments presents unique
challenges. Developers often need to analyze application behavior without
stopping or restarting services, which is especially crucial for
high-availability systems. Common scenarios include diagnosing deadlocks,
inspecting memory usage, or investigating unexpected behavior in real-time.

Very few Python tools can attach to running processes, primarily because doing
so requires deep expertise in both operating system debugging interfaces and
CPython internals. While C/C++ debuggers like GDB and LLDB can attach to
processes using well-understood techniques, Python tools must implement all of
these low-level mechanisms plus handle additional complexity. For example, when
GDB needs to execute code in a target process, it:

1. Uses ptrace to allocate a small chunk of executable memory (easier said than done)
2. Writes a small sequence of machine code - typically a function prologue, the
   desired instructions, and code to restore registers
3. Saves all the target thread's registers
4. Changes the instruction pointer to the injected code
5. Lets the process run until it hits a breakpoint at the end of the injected code
6. Restores the original registers and continues execution

Python tools face this same challenge of code injection, but with an additional
layer of complexity. Not only do they need to implement the above mechanism,
they must also understand and safely interact with CPython's runtime state,
including the interpreter loop, garbage collector, thread state, and reference
counting system. This combination of low-level system manipulation and
deep domain specific interpreter knowledge makes implementing Python debugging tools
exceptionally difficult.

The few tools (see for example `DebugPy
<https://github.com/microsoft/debugpy/blob/43f41029eabce338becbd1fa1a09727b3cfb1140/src/debugpy/_vendored/pydevd/pydevd_attach_to_process/linux_and_mac/attach.cpp#L4>`__
and `Memray
<https://github.com/bloomberg/memray/blob/main/src/memray/_memray/inject.cpp>`__)
that do attempt this resort to suboptimal and unsafe methods,
using system debuggers like GDB and LLDB to forcefully inject code. This
approach is fundamentally unsafe because the injected code can execute at any
point during the interpreter's execution cycle - even during critical operations
like memory allocation, garbage collection, or thread state management. When
this happens, the results are catastrophic: attempting to allocate memory while
already inside ``malloc()`` causes crashes, modifying objects during garbage
collection corrupts the interpreter's state, and touching thread state at the
wrong time leads to deadlocks.

Various tools attempt to minimize these risks through complex workarounds, such
as spawning separate threads for injected code or carefully timing their
operations or trying to select some good points to stop the process. However,
these mitigations cannot fully solve the underlying problem: without cooperation
from the interpreter, there's no way to know if it's safe to execute code at any
given moment. Even carefully implemented tools can crash the interpreter because
they're fundamentally working against it rather than with it.


Rationale
=========


Rather than forcing tools to work around interpreter limitations with unsafe
code injection, we can extend CPython with a proper debugging interface that
guarantees safe execution. By adding a few thread state fields and integrating
with the interpreter's existing evaluation loop, we can ensure debugging
operations only occur at well-defined safe points. This eliminates the
possibility of crashes and corruption while maintaining zero overhead during
normal execution.

The key insight is that we don't need to inject code at arbitrary points - we
just need to signal to the interpreter that we want code executed at the next
safe opportunity. This approach works with the interpreter's natural execution
flow rather than fighting against it.

After describing this idea to the PyPy development team, this proposal has
already `been implemented in PyPy <https://github.com/pypy/pypy/pull/5135>`__,
proving both its feasibility and effectiveness. Their implementation
demonstrates that we can provide safe debugging capabilities with zero runtime
overhead during normal execution.  The proposed mechanism not only reduces risks
associated with current debugging approaches but also lays the foundation for
future enhancements. For instance, this framework could enable integration with
popular observability tools, providing real-time insights into interpreter
performance or memory usage. One compelling use case for this interface is
enabling pdb to attach to running Python processes, similar to how gdb allows
users to attach to a program by process ID (``gdb -p <pid>``). With this
feature, developers could inspect the state of a running application, evaluate
expressions, and step through code dynamically. This approach would align
Python's debugging capabilities with those of other major programming languages
and debugging tools that support this mode.

Specification
=============


This proposal introduces a safe debugging mechanism that allows external
processes to trigger code execution in a Python interpreter at well-defined safe
points. The key insight is that rather than injecting code directly via system
debuggers, we can leverage the interpreter's existing evaluation loop and thread
state to coordinate debugging operations.

The mechanism works by having debuggers write to specific memory locations in
the target process that the interpreter then checks during its normal execution
cycle. When the interpreter detects that a debugger wants to attach, it executes the
requested operations only when it's safe to do so - that is, when no internal
locks are held and all data structures are in a consistent state.


Runtime State Extensions
------------------------

A new structure is added to PyThreadState to support remote debugging:

.. code-block:: C

    typedef struct _remote_debugger_support {
        int debugger_pending_call;
        char debugger_script_path[MAX_SCRIPT_PATH_SIZE];
    } _PyRemoteDebuggerSupport;

This structure is appended to ``PyThreadState``, adding only a few fields that
are **never accessed during normal execution**. The ``debugger_pending_call`` field
indicates when a debugger has requested execution, while ``debugger_script_path``
provides a filesystem path to a Python source file (.py) that will be executed when
the interpreter reaches a safe point. The path must point to a Python source file,
not compiled Python code (.pyc) or any other format.

The value for ``MAX_SCRIPT_PATH_SIZE`` will be a trade-off between binary size
and how big debugging scripts' paths can be. To limit the memory overhead per
thread we will be limiting this to 512 bytes. This size will also be provided as
part of the debugger support structure so debuggers know how much they can
write. This value can be extended in the future if we ever need to.


Debug Offsets Table
-------------------


Python 3.12 introduced a debug offsets table placed at the start of the
PyRuntime structure. This section contains the ``_Py_DebugOffsets`` structure that
allows external tools to reliably find critical runtime structures regardless of
`ASLR <https://en.wikipedia.org/wiki/Address_space_layout_randomization>`__ or
how Python was compiled.

This proposal extends the existing debug offsets table with new fields for
debugger support:

.. code-block:: C

    struct _debugger_support {
        uint64_t eval_breaker;              // Location of the eval breaker flag
        uint64_t remote_debugger_support;   // Offset to our support structure
        uint64_t debugger_pending_call;     // Where to write the pending flag
        uint64_t debugger_script_path;      // Where to write the script path
        uint64_t debugger_script_path_size; // Size of the script path buffer
    } debugger_support;

These offsets allow debuggers to locate critical debugging control structures in
the target process's memory space. The ``eval_breaker`` and ``remote_debugger_support``
offsets are relative to each ``PyThreadState``, while the ``debugger_pending_call``
and ``debugger_script_path`` offsets are relative to each ``_PyRemoteDebuggerSupport``
structure, allowing the new structure and its fields to be found regardless of
where they are in memory. ``debugger_script_path_size`` informs the attaching
tool of the size of the buffer.

Attachment Protocol
-------------------
When a debugger wants to attach to a Python process, it follows these steps:

1. Locate ``PyRuntime`` structure in the process:

   - Find Python binary (executable or libpython) in process memory (OS dependent process)
   - Extract ``.PyRuntime`` section offset from binary's format (ELF/Mach-O/PE)
   - Calculate the actual ``PyRuntime`` address in the running process by relocating the offset to the binary's load address

2. Access debug offset information by reading the ``_Py_DebugOffsets`` at the start of the ``PyRuntime`` structure.

3. Use the offsets to locate the desired thread state

4. Use the offsets to locate the debugger interface fields within that thread state

5. Write control information:

   - Most debuggers will pause the process before writing to its memory. This is
     standard practice for tools like GDB, which use SIGSTOP or ptrace to pause the process.
     This approach prevents races when writing to process memory. Profilers and other tools
     that don't wish to stop the process can still use this interface, but they need to
     handle possible races. This is a normal consideration for profilers.

   - Write a file path to a Python source file (.py) into the
     ``debugger_script_path`` field in ``_PyRemoteDebuggerSupport``.
   - Set ``debugger_pending_call`` flag in ``_PyRemoteDebuggerSupport`` to 1
   - Set ``_PY_EVAL_PLEASE_STOP_BIT`` in the ``eval_breaker`` field

Once the interpreter reaches the next safe point, it will execute the Python code
contained in the file specified by the debugger.

Interpreter Integration
-----------------------

The interpreter's regular evaluation loop already includes a check of the
``eval_breaker`` flag for handling signals, periodic tasks, and other interrupts. We
leverage this existing mechanism by checking for debugger pending calls only
when the ``eval_breaker`` is set, ensuring zero overhead during normal execution.
This check has no overhead. Indeed, profiling with Linux ``perf`` shows this branch
is highly predictable - the ``debugger_pending_call`` check is never taken during
normal execution, allowing modern CPUs to effectively speculate past it.


When a debugger has set both the ``eval_breaker`` flag and ``debugger_pending_call``,
the interpreter will execute the provided debugging code at the next safe point.
This all happens in a completely safe context, since the interpreter is
guaranteed to be in a consistent state whenever the eval breaker is checked.

The only valid values for ``debugger_pending_call`` will initially be 0 and 1
and other values are reserved for future use.

An audit event will be raised before the code is executed, allowing this mechanism
to be audited or disabled if desired by a system's administrator.

.. code-block:: c

    // In ceval.c
    if (tstate->eval_breaker) {
        if (tstate->remote_debugger_support.debugger_pending_call) {
            tstate->remote_debugger_support.debugger_pending_call = 0;
            const char *path = tstate->remote_debugger_support.debugger_script_path;
            if (*path) {
                if (0 != PySys_Audit("debugger_script", "%s", path)) {
                    PyErr_Clear();
                } else {
                    FILE* f = fopen(path, "r");
                    if (!f) {
                        PyErr_SetFromErrno(OSError);
                    } else {
                        PyRun_AnyFile(f, path);
                        fclose(f);
                    }
                    if (PyErr_Occurred()) {
                        PyErr_WriteUnraisable(...);
                    }
                }
            }
        }
    }


If the code being executed raises any Python exception it will be processed as
an `unraisable exception
<https://docs.python.org/3/c-api/exceptions.html#c.PyErr_WriteUnraisable>`__ in
the thread where the code was executed.

Python API
----------

To support safe execution of Python code in a remote process without having to
re-implement all these steps in every tool, this proposal extends the ``sys`` module
with a new function. This function allows debuggers or external tools to execute
arbitrary Python code within the context of a specified Python process:

.. code-block:: python

  def remote_exec(pid: int, script: str|bytes|PathLike) -> None:
      """
      Executes a file containing Python code in a given remote Python process.

      This function returns immediately, and the code will be executed by the
      target process's main thread at the next available opportunity, similarly
      to how signals are handled. There is no interface to determine when the
      code has been executed. The caller is responsible for making sure that
      the file still exists whenever the remote process tries to read it and that
      it hasn't been overwritten.

      Args:
           pid (int): The process ID of the target Python process.
           script (str|bytes|PathLike): The path to a file containing
               the Python code to be executed.
      """

An example usage of the API would look like:

.. code-block:: python

    import sys
    import uuid
    # Execute a print statement in a remote Python process with PID 12345
    script = f"/tmp/{uuid.uuid4()}.py"
    with open(script, "w") as f:
        f.write("print('Hello from remote execution!')")
    try:
        sys.remote_exec(12345, script)
    except Exception as e:
        print(f"Failed to execute code: {e}")

Configuration API
-----------------

To allow redistributors, system administrators, or users to disable this
mechanism, several methods will be provided to control the behavior of the
interpreter:

A new ``PYTHON_DISABLE_REMOTE_DEBUG`` environment variable will
be provided to control the behaviour at runtime. If set to any value (including an empty string), the
interpreter will ignore any attempts to attach a debugger using this mechanism.

This environment variable will be added together with a new ``-X disable-remote-debug``
flag to the Python interpreter to allow users to disable this feature at runtime.

Additionally a new  ``--without-remote-debug`` flag will be added to the
``configure`` script to allow redistributors to build Python without support for
remote debugging if they so desire.

A new flag indicating the status of remote debugging will be made available via
the debug offsets so tools can query if a remote process has disabled the
feature. This way, tools can offer a useful error message explaining why they
won't work, instead of believing that they have attached and then never having
their script run.

Multi-threading Considerations
------------------------------

The overall execution pattern resembles how Python handles signals internally.
The interpreter guarantees that injected code only runs at safe points, never
interrupting atomic operations within the interpreter itself. This approach
ensures that debugging operations cannot corrupt the interpreter state while
still providing timely execution in most real-world scenarios.

However, debugging code injected through this interface can execute in any
thread. This behavior is different than how Python handles signals, since
signal handlers can only run in the main thread. If a debugger wants to inject
code into every running thread, it must inject it into every ``PyThreadState``.
If a debugger wants to run code in the first available thread, it needs to
inject it into every ``PyThreadState``, and that injected code must check
whether it has already been run by another thread (likely by setting some flag
in the globals of some module).

Note that the Global Interpreter Lock (GIL) continues to govern execution as
normal when the injected code runs. This means if a target thread is currently
executing a C extension that holds the GIL continuously, the injected code
won't be able to run until that operation completes and the GIL becomes
available. However, the interface introduces no additional GIL contention
beyond what the injected code itself requires. Importantly, the interface
remains fully compatible with Python's free-threaded mode.

It may be useful for a debugger that injected some code to be run to follow
that up by sending some pre-registered signal to the process, which can
interrupt any blocking I/O or sleep states waiting for external resources, and
allow a safe opportunity to run the injected code.

Backwards Compatibility
=======================

This change has no impact on existing Python code or interpreter performance.
The added fields are only accessed during debugger attachment, and the checking
mechanism piggybacks on existing interpreter safe points.

Security Implications
=====================

This interface does not introduce new security concerns as it is only usable by
processes that can already write to arbitrary memory within a given process and
execute arbitrary code on the machine (in order to create the file containing
the Python code to be executed).

Furthermore, the execution of the code is gated by the interpreter's
audit hooks, which can be used to monitor or prevent the execution of the code
in sensitive environments.

Existing operating system security mechanisms are effective for guarding
against attackers gaining arbitrary memory write access. Although the PEP
doesn't specify how memory should be written to the target process, in practice
this will be done using standard system calls that are already being used by
other debuggers and tools. Some examples are:

* On Linux, the `process_vm_readv() <https://man7.org/linux/man-pages/man2/process_vm_readv.2.html>`__
  and `process_vm_writev() <https://man7.org/linux/man-pages/man2/process_vm_writev.2.html>`__ system calls
  are used to read and write memory from another process. These operations are
  controlled by `ptrace <https://man7.org/linux/man-pages/man2/ptrace.2.html>`__ access mode
  checks - the same ones that govern debugger attachment. A process can only read from
  or write to another process's memory if it has the appropriate permissions (typically
  requiring either root or the `CAP_SYS_PTRACE <https://man7.org/linux/man-pages/man7/capabilities.7.html>`__
  capability, though less security minded distributions may allow any process running as the same uid to attach).

* On macOS, the interface would leverage `mach_vm_read_overwrite() <https://developer.apple.com/documentation/kernel/1402127-mach_vm_read_overwrite>`__ and
  `mach_vm_write() <https://developer.apple.com/documentation/kernel/1402070-mach_vm_write>`__ through the Mach task system. These operations require
  ``task_for_pid()`` access, which is strictly controlled by the operating
  system. By default, access is limited to processes running as root or those
  with specific entitlements granted by Apple's security framework.

* On Windows, the `ReadProcessMemory() <https://learn.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-readprocessmemory>`__
  and `WriteProcessMemory() <https://learn.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-writeprocessmemory>`__ functions
  provide similar functionality. Access is controlled through the Windows
  security model - a process needs `PROCESS_VM_READ <https://learn.microsoft.com/en-us/windows/win32/procthread/process-security-and-access-rights>`__
  and `PROCESS_VM_WRITE <https://learn.microsoft.com/en-us/windows/win32/procthread/process-security-and-access-rights>`__
  permissions, which typically require the same user context or appropriate
  privileges. These are the same permissions required by debuggers, ensuring
  consistent security semantics across platforms.

All mechanisms ensure that:

1. Only authorized processes can read/write memory
2. The same security model that governs traditional debugger attachment applies
3. No additional attack surface is exposed beyond what the OS already provides for debugging
4. Even if an attacker can write arbitrary memory, they cannot escalate this
   to arbitrary code execution unless they already have filesystem access

The memory operations themselves are well-established and have been used safely
for decades in tools like GDB, LLDB, and various system profilers.

It's important to note that any attempt to attach to a Python process via this
mechanism would be detectable by system-level monitoring tools as well as by
Python audit hooks. This transparency provides an additional layer of
accountability, allowing administrators to audit debugging operations in
sensitive environments.

Further, the strict reliance on OS-level security controls ensures that existing
system policies remain effective. For enterprise environments, this means
administrators can continue to enforce debugging restrictions using standard
tools and policies without requiring additional configuration. For instance,
leveraging Linux's `ptrace_scope <https://www.kernel.org/doc/Documentation/security/Yama.txt>`__
or macOS's ``taskgated`` to restrict debugger access will equally govern the
proposed interface.

By maintaining compatibility with existing security frameworks, this design
ensures that adopting the new interface requires no changes to established.

Security scenarios
------------------

* For an external attacker, the ability to write to arbitrary memory in a
  process is already a severe security issue. This interface does not introduce
  any new attack surface, as the attacker would already have the ability to
  execute arbitrary code in the process. This interface behaves in exactly
  the same way as existing debuggers, and does not introduce any new additional
  security risks.
* For an attacker who has gained arbitrary memory write access to a process but
  not arbitrary code execution, this interface does not allow them to escalate.
  The ability to calculate and write to specific memory locations is required,
  which is not available without compromising other machine resources that
  are external to the Python process.

Additionally, the fact that the code to be executed is gated by the interpreter's
audit hooks means that the execution of the code can be monitored and controlled
by system administrators. This means that even if the attacker has compromised the
application **and the filesystem**, leveraging this interface for malicious
purposes provides a very risky proposition for an attacker, as they risk
exposing their actions to system administrators that could not only detect the
attack but also take action to prevent it.

Finally, is important to note that if an attacker has arbitrary memory write
access to a process and has compromised the filesystem, they can already
escalate to arbitrary code execution using other existing mechanisms, so this
interface does not introduce any new risks in this scenario.


How to Teach This
=================

For tool authors, this interface becomes the standard way to implement debugger
attachment, replacing unsafe system debugger approaches. A section in the Python
Developer Guide could describe the internal workings of the mechanism, including
the ``debugger_support`` offsets and how to interact with them using system
APIs.

End users need not be aware of the interface, benefiting only from improved
debugging tool stability and reliability.

Reference Implementation
========================

A reference implementation with a prototype adding remote support for ``pdb``
can be found `here
<https://github.com/pablogsal/cpython/compare/60ff67d010078eca15a74b1429caf779ac4f9c74...remote_pdb>`__.

Rejected Ideas
==============

Writing Python code into the buffer
-----------------------------------

We have chosen to have debuggers write the path to a file containing Python code
into a buffer in the remote process. This has been deemed
more secure than writing the Python code to be executed itself into a buffer in
the remote process, because it means that an attacker who has gained arbitrary
writes in a process but not arbitrary code execution or file system
manipulation can't escalate to arbitrary code execution through this interface.

This does require the attaching debugger to pay close attention to filesystem
permissions when creating the file containing the code to be executed, however.
If an attacker has the ability to overwrite the file, or to replace a symlink
in the file path to point to somewhere attacker controlled, this would allow
them to force their malicious code to be executed rather than the code the
debugger intends to run.

Using a Single Runtime Buffer
-----------------------------

During the review of this PEP it has been suggested using a single
shared buffer at the runtime level for all debugger communications. While this
appeared simpler and required less memory, we discovered it would actually prevent scenarios
where multiple debuggers need to coordinate operations across different threads,
or where a single debugger needs to orchestrate complex debugging operations. A
single shared buffer would force serialization of all debugging operations,
making it impossible for debuggers to work independently on different threads.

The per-thread buffer approach, despite its memory overhead in highly threaded
applications, enables these important debugging scenarios by allowing each
debugger to communicate independently with its target thread.

Thanks
======

We would like to thank CF Bolz-Tereick for their insightful comments and suggestions
when discussing this proposal.


Copyright
=========

This document is placed in the public domain or under the CC0-1.0-Universal
license, whichever is more permissive.
